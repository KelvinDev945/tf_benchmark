# TensorFlow Multi-Engine CPU Inference Benchmark Configuration

# Benchmark execution parameters
benchmark:
  # Number of warmup iterations before actual testing
  warmup_iterations: 50

  # Number of test iterations for measurement
  test_iterations: 200

  # Number of times to repeat the entire benchmark (take median)
  repeat_runs: 5

  # Confidence level for statistical intervals
  confidence_level: 0.95

  # Enable detailed logging
  verbose: true

  # Save intermediate checkpoints
  enable_checkpoints: true

  # Checkpoint save interval (number of completed tests)
  checkpoint_interval: 10

# Dataset configuration
dataset:
  # Image dataset settings
  image:
    # HuggingFace dataset name
    name: "imagenet-1k"

    # Dataset split to use
    split: "validation"

    # Number of samples to use (null for all)
    num_samples: 5000

    # Target image size (height, width)
    target_size: [224, 224]

    # Enable caching
    cache: true

    # Cache directory
    cache_dir: "./data/cache/imagenet"

  # Lightweight text dataset (in-memory) for optional BERT benchmarks
  text:
    name: "builtin-sst2"
    subset: "validation"
    split: "validation"
    num_samples: 512
    max_length: 128

# Models to benchmark
models:
  # Image classification models
  image:
    - mobilenet_v2
    - resnet50
    - efficientnet_b0

  # Text understanding models (TensorFlow Hub)
  text:
    - bert-base-uncased

# Batch sizes to test
batch_sizes: [1, 4, 8, 16, 32]

# Sequence lengths to evaluate for text models (used in extended benchmarks)
sequence_lengths: [64, 128, 256, 512]

# Inference engines and their configurations
engines:
  # TensorFlow configurations
  tensorflow:
    enabled: true
    configs:
      # Baseline: default TensorFlow settings
      - name: baseline
        description: "Default TensorFlow configuration"
        xla: false
        mixed_precision: false
        inter_op_threads: null  # Auto-detect
        intra_op_threads: null  # Auto-detect

      # XLA: Enable XLA JIT compilation
      - name: xla
        description: "TensorFlow with XLA JIT compilation"
        xla: true
        mixed_precision: false
        inter_op_threads: null
        intra_op_threads: null

      # Threads: Optimized thread configuration
      - name: threads
        description: "TensorFlow with optimized threading"
        xla: false
        mixed_precision: false
        inter_op_threads: 4
        intra_op_threads: 8

      # Mixed Precision: BFloat16
      - name: mixed_precision
        description: "TensorFlow with mixed precision (BFloat16)"
        xla: false
        mixed_precision: true
        inter_op_threads: null
        intra_op_threads: null

      # Best Combo: All optimizations combined
      - name: best_combo
        description: "TensorFlow with XLA + threading + mixed precision"
        xla: true
        mixed_precision: true
        inter_op_threads: 4
        intra_op_threads: 8

  # TFLite configurations (quantization modes)
  tflite:
    enabled: true
    configs:
      # Float32: No quantization
      - name: float32
        description: "TFLite without quantization"
        optimization: "NONE"
        num_threads: 4

      # Dynamic Range: Dynamic range quantization
      - name: dynamic_range
        description: "TFLite with dynamic range quantization"
        optimization: "DYNAMIC_RANGE"
        num_threads: 4

      # INT8: Full integer quantization
      - name: int8
        description: "TFLite with full INT8 quantization"
        optimization: "INT8"
        num_threads: 4
        calibration_samples: 100

      # Float16: Half precision quantization
      - name: float16
        description: "TFLite with Float16 quantization"
        optimization: "FLOAT16"
        num_threads: 4

  # ONNX Runtime configurations
  onnxruntime:
    enabled: true
    configs:
      # Default: Standard ONNX Runtime settings
      - name: default
        description: "ONNX Runtime with default settings"
        graph_optimization_level: "ENABLE_BASIC"
        inter_op_num_threads: null
        intra_op_num_threads: null
        execution_mode: "SEQUENTIAL"

      # Optimized: Graph optimization + parallelism
      - name: optimized
        description: "ONNX Runtime with full optimizations"
        graph_optimization_level: "ENABLE_ALL"
        inter_op_num_threads: 4
        intra_op_num_threads: 8
        execution_mode: "PARALLEL"

      # Quantized: INT8 quantization
      - name: quantized
        description: "ONNX Runtime with INT8 quantization"
        graph_optimization_level: "ENABLE_ALL"
        inter_op_num_threads: 4
        intra_op_num_threads: 8
        execution_mode: "PARALLEL"
        quantize: true

  # OpenVINO configurations (x86_64 only)
  openvino:
    enabled: true  # Will be auto-disabled on ARM64
    configs:
      # FP32: Single precision floating point
      - name: fp32
        description: "OpenVINO with FP32 precision"
        precision: "FP32"
        performance_hint: "THROUGHPUT"
        num_streams: 4

      # FP16: Half precision floating point
      - name: fp16
        description: "OpenVINO with FP16 precision"
        precision: "FP16"
        performance_hint: "THROUGHPUT"
        num_streams: 4

      # INT8: Integer quantization
      - name: int8
        description: "OpenVINO with INT8 quantization"
        precision: "INT8"
        performance_hint: "THROUGHPUT"
        num_streams: 4

      # Dynamic: Dynamic batching optimized
      - name: dynamic
        description: "OpenVINO with dynamic batching"
        precision: "FP32"
        performance_hint: "LATENCY"
        num_streams: 1
        dynamic_batching: true

# Output configuration
output:
  # Results directory
  results_dir: "./results"

  # Save results in CSV format
  save_csv: true

  # Save results in JSON format
  save_json: true

  # Generate plots
  generate_plots: true

  # Generate HTML report
  generate_report: true

  # Plot format
  plot_format: "png"  # png, svg, or both

  # Plot DPI
  plot_dpi: 300

  # Report template
  report_template: "default"

# Logging configuration
logging:
  # Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  level: "INFO"

  # Log to console
  console: true

  # Log to file
  file: true

  # Log file path
  log_file: "./results/benchmark.log"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Use colored logs
  colored: true

# Resource monitoring configuration
monitoring:
  # Enable resource monitoring
  enabled: true

  # Sampling interval in seconds
  sampling_interval: 0.1

  # Monitor CPU usage
  monitor_cpu: true

  # Monitor memory usage
  monitor_memory: true

  # Monitor per-core CPU usage
  monitor_per_core: true

# Model conversion configuration
conversion:
  # Directory to save converted models
  model_dir: "./models/converted"

  # Keep intermediate files
  keep_intermediate: true

  # Verify conversion accuracy
  verify_accuracy: true

  # Maximum allowed accuracy loss (percentage)
  max_accuracy_loss: 5.0

  # Number of samples for verification
  verification_samples: 100

# Testing modes (can be selected via CLI)
modes:
  # Quick mode: Fast verification (for development)
  quick:
    warmup_iterations: 10
    test_iterations: 50
    repeat_runs: 1
    batch_sizes: [1, 8]
    num_samples_image: 100
    num_samples_text: 50

  # Standard mode: Balanced testing (default)
  standard:
    warmup_iterations: 50
    test_iterations: 200
    repeat_runs: 3
    batch_sizes: [1, 4, 8, 16, 32]
    num_samples_image: 2000
    num_samples_text: 200
    sequence_lengths: [64, 128, 256]
  # Full mode: Comprehensive testing (for publication)
  full:
    warmup_iterations: 100
    test_iterations: 500
    repeat_runs: 5
    batch_sizes: [1, 2, 4, 8, 16, 32, 64]
    num_samples_image: 5000
    num_samples_text: 1000
