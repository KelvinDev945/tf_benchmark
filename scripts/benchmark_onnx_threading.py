#!/usr/bin/env python3
"""
ONNX Runtimeå¤šçº¿ç¨‹é…ç½®æ€§èƒ½æµ‹è¯•

æµ‹è¯•ONNX Runtimeåœ¨ä¸åŒçº¿ç¨‹é…ç½®ä¸‹çš„æ€§èƒ½
å¯¹æ¯”TensorFlowå¤šçº¿ç¨‹ä¼˜åŒ–å vs ONNX Runtimeå¤šçº¿ç¨‹ä¼˜åŒ–
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
import numpy as np
import tensorflow as tf
import onnxruntime as ort
import multiprocessing

def print_section(title):
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)

def get_cpu_info():
    """è·å–CPUæ ¸å¿ƒæ•°ä¿¡æ¯"""
    physical_cores = multiprocessing.cpu_count()
    try:
        with open('/proc/cpuinfo', 'r') as f:
            cpuinfo = f.read()
            physical_ids = set()
            for line in cpuinfo.split('\n'):
                if line.startswith('physical id'):
                    physical_ids.add(line.split(':')[1].strip())
            cores_per_cpu = 0
            for line in cpuinfo.split('\n'):
                if line.startswith('cpu cores'):
                    cores_per_cpu = int(line.split(':')[1].strip())
                    break
            actual_physical_cores = len(physical_ids) * cores_per_cpu if physical_ids else physical_cores
    except:
        actual_physical_cores = physical_cores

    return {
        'logical_cores': physical_cores,
        'physical_cores': actual_physical_cores,
        'hyperthreading': physical_cores > actual_physical_cores
    }

def create_bert_base_model():
    """åˆ›å»ºBERT-Baseæ¨¡å‹"""
    hidden_size = 768
    num_hidden_layers = 12
    num_attention_heads = 12
    seq_length = 128
    vocab_size = 10000
    intermediate_size = hidden_size * 4

    input_ids = tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32, name='input_ids')
    embeddings = tf.keras.layers.Embedding(vocab_size, hidden_size, name='embedding')(input_ids)
    position_embeddings = tf.keras.layers.Embedding(seq_length, hidden_size, name='position_embedding')(tf.range(seq_length))

    x = embeddings + position_embeddings
    x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x)
    x = tf.keras.layers.Dropout(0.1)(x)

    for i in range(num_hidden_layers):
        attention_output = tf.keras.layers.MultiHeadAttention(
            num_heads=num_attention_heads,
            key_dim=hidden_size // num_attention_heads,
            name=f'attention_{i}'
        )(x, x)
        attention_output = tf.keras.layers.Dropout(0.1)(attention_output)
        x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x + attention_output)

        ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(intermediate_size, activation='relu'),
            tf.keras.layers.Dense(hidden_size)
        ], name=f'ffn_{i}')
        ffn_output = ffn(x)
        ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)
        x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x + ffn_output)

    pooled_output = tf.keras.layers.Lambda(lambda x: x[:, 0])(x)
    pooled_output = tf.keras.layers.Dense(hidden_size, activation='tanh', name='pooler')(pooled_output)
    output = tf.keras.layers.Dense(2, activation='softmax', name='classifier')(pooled_output)

    model = tf.keras.Model(inputs=input_ids, outputs=output, name='bert_base_model')
    return model

def create_mobilenet_model():
    """åˆ›å»ºMobileNetV2æ¨¡å‹"""
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=(224, 224, 3),
        include_top=False,
        weights=None
    )
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(1000, activation='softmax')
    ], name='mobilenet_v2')
    return model

def convert_to_onnx(model, output_path, model_type):
    """è½¬æ¢TensorFlowæ¨¡å‹åˆ°ONNX"""
    import subprocess
    import tempfile

    # ä¿å­˜ä¸ºSavedModel
    temp_dir = tempfile.mkdtemp()
    saved_model_path = os.path.join(temp_dir, "saved_model")
    model.export(saved_model_path)

    # è½¬æ¢ä¸ºONNX
    cmd = [
        "python3", "-m", "tf2onnx.convert",
        "--saved-model", saved_model_path,
        "--output", output_path,
        "--opset", "13"
    ]

    subprocess.run(cmd, check=True, capture_output=True)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)

def benchmark_onnx_with_threads(onnx_path, X_test, intra_threads, inter_threads,
                                num_runs=30, batch_size=1):
    """
    ä½¿ç”¨æŒ‡å®šçº¿ç¨‹é…ç½®æµ‹è¯•ONNX Runtimeæ€§èƒ½

    Args:
        onnx_path: ONNXæ¨¡å‹è·¯å¾„
        X_test: æµ‹è¯•æ•°æ®
        intra_threads: intra_opçº¿ç¨‹æ•°
        inter_threads: inter_opçº¿ç¨‹æ•°
        num_runs: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    # é…ç½®ONNX Runtime Session Options
    sess_options = ort.SessionOptions()

    # è®¾ç½®çº¿ç¨‹é…ç½®
    if intra_threads > 0:
        sess_options.intra_op_num_threads = intra_threads
    if inter_threads > 0:
        sess_options.inter_op_num_threads = inter_threads

    # å…¶ä»–ä¼˜åŒ–é€‰é¡¹
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    # åˆ›å»ºæ¨ç†session
    session = ort.InferenceSession(onnx_path, sess_options)

    input_name = session.get_inputs()[0].name

    # çƒ­èº«
    num_warmup = 5
    for _ in range(num_warmup):
        _ = session.run(None, {input_name: X_test[:batch_size]})

    # æ€§èƒ½æµ‹è¯•
    latencies = []
    for _ in range(num_runs):
        start = time.perf_counter()
        _ = session.run(None, {input_name: X_test[:batch_size]})
        latency = (time.perf_counter() - start) * 1000
        latencies.append(latency)

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    results = {
        "intra_threads": intra_threads if intra_threads > 0 else "default",
        "inter_threads": inter_threads if inter_threads > 0 else "default",
        "batch_size": batch_size,
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "std_ms": float(np.std(latencies)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": float(batch_size * 1000.0 / np.mean(latencies))
    }

    return results

def test_onnx_threading_configs(onnx_path, X_test, model_name, cpu_info, num_runs=30):
    """æµ‹è¯•å¤šç§ONNX Runtimeçº¿ç¨‹é…ç½®"""
    print_section(f"æµ‹è¯• {model_name} ONNX - ä¸åŒçº¿ç¨‹é…ç½®")

    logical_cores = cpu_info['logical_cores']
    physical_cores = cpu_info['physical_cores']

    print(f"CPUä¿¡æ¯:")
    print(f"  é€»è¾‘æ ¸å¿ƒæ•°: {logical_cores}")
    print(f"  ç‰©ç†æ ¸å¿ƒæ•°: {physical_cores}")

    # æµ‹è¯•é…ç½®åˆ—è¡¨
    test_configs = [
        (1, 1, "å•çº¿ç¨‹"),
        (2, 1, "2çº¿ç¨‹ (intra)"),
        (4, 1, "4çº¿ç¨‹ (intra)"),
        (8, 1, "8çº¿ç¨‹ (intra)"),
        (physical_cores, 1, f"{physical_cores}çº¿ç¨‹ (ç‰©ç†æ ¸å¿ƒ)"),
        (physical_cores, 2, f"{physical_cores}çº¿ç¨‹ + 2 inter"),
        (0, 0, "é»˜è®¤é…ç½®"),
    ]

    results = []

    for intra, inter, desc in test_configs:
        print(f"\næµ‹è¯•é…ç½®: {desc}")
        print(f"  intra_op_num_threads: {intra if intra > 0 else 'default'}")
        print(f"  inter_op_num_threads: {inter if inter > 0 else 'default'}")

        result = benchmark_onnx_with_threads(
            onnx_path, X_test, intra, inter, num_runs=num_runs
        )
        result['description'] = desc
        results.append(result)

        print(f"  âœ“ å¹³å‡å»¶è¿Ÿ: {result['mean_ms']:.2f} ms")
        print(f"  âœ“ ååé‡: {result['throughput_samples_per_sec']:.2f} samples/sec")

    return results

def generate_report(bert_results, mobilenet_results, cpu_info, output_file):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    print_section("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")

    # æ‰¾åˆ°æœ€ä¼˜é…ç½®
    bert_optimal = min(bert_results, key=lambda x: x['mean_ms'])
    mobilenet_optimal = min(mobilenet_results, key=lambda x: x['mean_ms'])

    bert_baseline = next(r for r in bert_results if r['intra_threads'] == 1)
    mobilenet_baseline = next(r for r in mobilenet_results if r['intra_threads'] == 1)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ONNX Runtimeå¤šçº¿ç¨‹é…ç½®æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## ç³»ç»Ÿä¿¡æ¯\n\n")
        f.write(f"- ONNX Runtimeç‰ˆæœ¬: {ort.__version__}\n")
        f.write(f"- CPUé€»è¾‘æ ¸å¿ƒæ•°: {cpu_info['logical_cores']}\n")
        f.write(f"- CPUç‰©ç†æ ¸å¿ƒæ•°: {cpu_info['physical_cores']}\n")
        f.write(f"- è¶…çº¿ç¨‹: {'å¯ç”¨' if cpu_info['hyperthreading'] else 'ç¦ç”¨'}\n\n")

        # BERT-Baseç»“æœ
        f.write("## BERT-Base ONNX çº¿ç¨‹é…ç½®æµ‹è¯•\n\n")
        f.write("| é…ç½® | Intraçº¿ç¨‹ | Interçº¿ç¨‹ | å¹³å‡å»¶è¿Ÿ | P95å»¶è¿Ÿ | ååé‡ | vså•çº¿ç¨‹ |\n")
        f.write("|------|-----------|-----------|----------|---------|--------|----------|\n")

        for result in bert_results:
            speedup = bert_baseline['mean_ms'] / result['mean_ms']
            intra_str = str(result['intra_threads'])
            inter_str = str(result['inter_threads'])

            f.write(f"| {result['description']} | {intra_str} | {inter_str} | ")
            f.write(f"{result['mean_ms']:.2f} ms | {result['p95_ms']:.2f} ms | ")
            f.write(f"{result['throughput_samples_per_sec']:.2f} samples/s | ")
            f.write(f"{speedup:.2f}x {'ğŸš€' if speedup > 1.2 else ''} |\n")

        f.write(f"\n**æœ€ä¼˜é…ç½®**: {bert_optimal['description']}\n")
        f.write(f"- å»¶è¿Ÿ: {bert_optimal['mean_ms']:.2f} ms\n")
        f.write(f"- ç›¸å¯¹å•çº¿ç¨‹åŠ é€Ÿ: {bert_baseline['mean_ms'] / bert_optimal['mean_ms']:.2f}x\n\n")

        # MobileNetç»“æœ
        f.write("## MobileNetV2 ONNX çº¿ç¨‹é…ç½®æµ‹è¯•\n\n")
        f.write("| é…ç½® | Intraçº¿ç¨‹ | Interçº¿ç¨‹ | å¹³å‡å»¶è¿Ÿ | P95å»¶è¿Ÿ | ååé‡ | vså•çº¿ç¨‹ |\n")
        f.write("|------|-----------|-----------|----------|---------|--------|----------|\n")

        for result in mobilenet_results:
            speedup = mobilenet_baseline['mean_ms'] / result['mean_ms']
            intra_str = str(result['intra_threads'])
            inter_str = str(result['inter_threads'])

            f.write(f"| {result['description']} | {intra_str} | {inter_str} | ")
            f.write(f"{result['mean_ms']:.2f} ms | {result['p95_ms']:.2f} ms | ")
            f.write(f"{result['throughput_samples_per_sec']:.2f} samples/s | ")
            f.write(f"{speedup:.2f}x {'ğŸš€' if speedup > 1.2 else ''} |\n")

        f.write(f"\n**æœ€ä¼˜é…ç½®**: {mobilenet_optimal['description']}\n")
        f.write(f"- å»¶è¿Ÿ: {mobilenet_optimal['mean_ms']:.2f} ms\n")
        f.write(f"- ç›¸å¯¹å•çº¿ç¨‹åŠ é€Ÿ: {mobilenet_baseline['mean_ms'] / mobilenet_optimal['mean_ms']:.2f}x\n\n")

        # æ€»ç»“
        f.write("## æ€»ç»“\n\n")

        bert_speedup = bert_baseline['mean_ms'] / bert_optimal['mean_ms']
        mobilenet_speedup = mobilenet_baseline['mean_ms'] / mobilenet_optimal['mean_ms']

        f.write("### å¤šçº¿ç¨‹æ€§èƒ½æå‡\n\n")
        f.write(f"- **BERT-Base**: {bert_speedup:.2f}x åŠ é€Ÿ (å•çº¿ç¨‹ â†’ {bert_optimal['description']})\n")
        f.write(f"- **MobileNetV2**: {mobilenet_speedup:.2f}x åŠ é€Ÿ (å•çº¿ç¨‹ â†’ {mobilenet_optimal['description']})\n\n")

        f.write("### æ¨èé…ç½®\n\n")
        f.write("**BERT-Base ONNXæ¨è**:\n```python\n")
        f.write("sess_options = ort.SessionOptions()\n")
        f.write(f"sess_options.intra_op_num_threads = {bert_optimal['intra_threads']}\n")
        f.write(f"sess_options.inter_op_num_threads = {bert_optimal['inter_threads']}\n")
        f.write("```\n\n")

        f.write("**MobileNetV2 ONNXæ¨è**:\n```python\n")
        f.write("sess_options = ort.SessionOptions()\n")
        f.write(f"sess_options.intra_op_num_threads = {mobilenet_optimal['intra_threads']}\n")
        f.write(f"sess_options.inter_op_num_threads = {mobilenet_optimal['inter_threads']}\n")
        f.write("```\n\n")

        f.write("### å…³é”®å‘ç°\n\n")
        if bert_speedup > 1.5:
            f.write("- âœ… BERT-Base ONNXå—ç›Šäºå¤šçº¿ç¨‹é…ç½®\n")
        else:
            f.write("- âš ï¸ BERT-Base ONNXé»˜è®¤é…ç½®å·²æ¥è¿‘æœ€ä¼˜\n")

        if mobilenet_speedup > 1.5:
            f.write("- âœ… MobileNetV2 ONNXå—ç›Šäºå¤šçº¿ç¨‹é…ç½®\n")
        else:
            f.write("- âš ï¸ MobileNetV2 ONNXé»˜è®¤é…ç½®å·²æ¥è¿‘æœ€ä¼˜\n")

        f.write("\n### ä¸TensorFlowå¯¹æ¯”\n\n")
        f.write("å‚è€ƒTensorFlowå¤šçº¿ç¨‹æµ‹è¯•ç»“æœï¼š\n")
        f.write("- TensorFlow BERT (auto): ~161ms â†’ ONNXæœ€ä¼˜: éœ€æµ‹è¯•éªŒè¯\n")
        f.write("- TensorFlow MobileNet (å•çº¿ç¨‹): ~82ms â†’ ONNXæœ€ä¼˜: éœ€æµ‹è¯•éªŒè¯\n\n")

        f.write("## å‚è€ƒ\n\n")
        f.write("- [ONNX Runtimeæ€§èƒ½è°ƒä¼˜](https://onnxruntime.ai/docs/performance/tune-performance.html)\n")
        f.write("- [TensorFlowå¤šçº¿ç¨‹æµ‹è¯•ç»“æœ](results/threading_benchmark/threading_benchmark_report.md)\n")

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="ONNX Runtimeå¤šçº¿ç¨‹é…ç½®æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--output-dir", default="results/onnx_threading",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num-runs", type=int, default=30,
                       help="æ¯ä¸ªé…ç½®çš„æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print_section("ONNX Runtimeå¤šçº¿ç¨‹é…ç½®æ€§èƒ½æµ‹è¯•")
    print(f"ONNX Runtimeç‰ˆæœ¬: {ort.__version__}")

    # è·å–CPUä¿¡æ¯
    cpu_info = get_cpu_info()

    # åˆ›å»ºå’Œè½¬æ¢BERTæ¨¡å‹
    print_section("å‡†å¤‡BERT-Baseæ¨¡å‹")
    bert_model = create_bert_base_model()
    bert_onnx_path = output_dir / "bert_base.onnx"

    if not bert_onnx_path.exists():
        print("è½¬æ¢BERTæ¨¡å‹åˆ°ONNX...")
        convert_to_onnx(bert_model, str(bert_onnx_path), "bert")
    print(f"âœ“ BERT ONNXæ¨¡å‹: {bert_onnx_path}")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    bert_X = np.random.randint(0, 10000, size=(200, 128), dtype=np.int32)

    # åˆ›å»ºå’Œè½¬æ¢MobileNetæ¨¡å‹
    print_section("å‡†å¤‡MobileNetV2æ¨¡å‹")
    mobilenet_model = create_mobilenet_model()
    mobilenet_onnx_path = output_dir / "mobilenet_v2.onnx"

    if not mobilenet_onnx_path.exists():
        print("è½¬æ¢MobileNetæ¨¡å‹åˆ°ONNX...")
        convert_to_onnx(mobilenet_model, str(mobilenet_onnx_path), "mobilenet")
    print(f"âœ“ MobileNet ONNXæ¨¡å‹: {mobilenet_onnx_path}")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    mobilenet_X = np.random.randn(200, 224, 224, 3).astype(np.float32)

    # æµ‹è¯•BERT ONNX
    bert_results = test_onnx_threading_configs(
        str(bert_onnx_path), bert_X, "BERT-Base",
        cpu_info, num_runs=args.num_runs
    )

    # æµ‹è¯•MobileNet ONNX
    mobilenet_results = test_onnx_threading_configs(
        str(mobilenet_onnx_path), mobilenet_X, "MobileNetV2",
        cpu_info, num_runs=args.num_runs
    )

    # ä¿å­˜åŸå§‹ç»“æœ
    results_json = output_dir / "results.json"
    with open(results_json, 'w', encoding='utf-8') as f:
        json.dump({
            "cpu_info": cpu_info,
            "bert_results": bert_results,
            "mobilenet_results": mobilenet_results,
            "config": {
                "num_runs": args.num_runs,
            }
        }, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ åŸå§‹ç»“æœå·²ä¿å­˜: {results_json}")

    # ç”ŸæˆæŠ¥å‘Š
    report_path = output_dir / "onnx_threading_report.md"
    generate_report(bert_results, mobilenet_results, cpu_info, report_path)

    # æ‰“å°æ€»ç»“
    print_section("âœ“ æµ‹è¯•å®Œæˆ!")

    bert_optimal = min(bert_results, key=lambda x: x['mean_ms'])
    mobilenet_optimal = min(mobilenet_results, key=lambda x: x['mean_ms'])

    bert_baseline = next(r for r in bert_results if r['intra_threads'] == 1)
    mobilenet_baseline = next(r for r in mobilenet_results if r['intra_threads'] == 1)

    print(f"\nBERT-Base ONNXæœ€ä¼˜é…ç½®: {bert_optimal['description']}")
    print(f"  å»¶è¿Ÿ: {bert_optimal['mean_ms']:.2f} ms")
    print(f"  åŠ é€Ÿæ¯”: {bert_baseline['mean_ms'] / bert_optimal['mean_ms']:.2f}x")

    print(f"\nMobileNetV2 ONNXæœ€ä¼˜é…ç½®: {mobilenet_optimal['description']}")
    print(f"  å»¶è¿Ÿ: {mobilenet_optimal['mean_ms']:.2f} ms")
    print(f"  åŠ é€Ÿæ¯”: {mobilenet_baseline['mean_ms'] / mobilenet_optimal['mean_ms']:.2f}x")

    print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_path}")

if __name__ == "__main__":
    main()
