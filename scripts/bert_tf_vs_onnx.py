#!/usr/bin/env python3
"""
BERT TensorFlow vs ONNX Runtime Performance Comparison (Fixed Version)

ä¿®å¤ç‰ˆæœ¬ - ä½¿ç”¨SavedModelç›´æ¥åŠ è½½ï¼Œé¿å…TF Hub KerasLayeré—®é¢˜
æ”¯æŒONNX Runtimeå¯¹æ¯”æµ‹è¯•
"""

import argparse
import json
import os
import time
from pathlib import Path

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print("="*70)
print("BERT CPUæ¨ç†æ€§èƒ½å¯¹æ¯”: TensorFlow vs ONNX Runtime (Fixed)")
print("="*70)
print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
print(f"NumPy ç‰ˆæœ¬: {np.__version__}")

try:
    import onnxruntime as ort
    print(f"ONNX Runtime ç‰ˆæœ¬: {ort.__version__}")
    ONNX_AVAILABLE = True
except ImportError:
    print("ONNX Runtime: Not installed")
    ONNX_AVAILABLE = False

print()


def parse_args():
    parser = argparse.ArgumentParser(description="BERT TensorFlow vs ONNX Benchmark (Fixed)")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size")
    parser.add_argument("--seq-length", type=int, default=128, help="Sequence length")
    parser.add_argument("--num-warmup", type=int, default=10, help="Warmup iterations")
    parser.add_argument("--num-test", type=int, default=50, help="Test iterations")
    parser.add_argument("--output", type=str, default="./results/bert_tf_vs_onnx_fixed", help="Output directory")
    parser.add_argument("--use-saved-model", action="store_true", help="Use SavedModel instead of KerasLayer")
    return parser.parse_args()


def create_test_data(num_samples, seq_length, batch_size=1):
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æµ‹è¯•æ•°æ®"""
    print(f"\nåˆ›å»ºæµ‹è¯•æ•°æ®...")
    print(f"  æ ·æœ¬æ•°: {num_samples}")
    print(f"  åºåˆ—é•¿åº¦: {seq_length}")
    print(f"  Batch size: {batch_size}")

    # ç”Ÿæˆéšæœºtoken IDs (BERT vocab sizeçº¦ä¸º30000)
    input_word_ids = np.random.randint(0, 30000, size=(num_samples, seq_length), dtype=np.int32)
    input_mask = np.ones((num_samples, seq_length), dtype=np.int32)
    input_type_ids = np.zeros((num_samples, seq_length), dtype=np.int32)

    print(f"âœ“ æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ")

    return {
        "input_word_ids": input_word_ids,
        "input_mask": input_mask,
        "input_type_ids": input_type_ids,
    }


def load_bert_with_savedmodel(model_cache_dir, seq_length):
    """
    æ–¹æ¡ˆ4: ä½¿ç”¨SavedModelç›´æ¥åŠ è½½BERT
    é¿å…KerasLayerçš„KerasTensoré—®é¢˜
    """
    print("ä½¿ç”¨SavedModelæ–¹å¼åŠ è½½BERT...")

    try:
        import tensorflow_hub as hub

        model_url = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"

        # ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
        print(f"ä¸‹è½½æ¨¡å‹: {model_url}")
        print("(é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ï¼Œçº¦440MBï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")

        # ä½¿ç”¨hub.resolve()è·å–æœ¬åœ°è·¯å¾„
        model_path = hub.resolve(model_url)
        print(f"âœ“ æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_path}")

        # ç›´æ¥åŠ è½½SavedModel
        bert_model = tf.saved_model.load(model_path)
        print("âœ“ BERT SavedModel åŠ è½½æˆåŠŸ")

        # è·å–ç­¾å
        serving_fn = bert_model.signatures['serving_default']
        print(f"âœ“ è·å–servingç­¾å")
        print(f"  è¾“å…¥: {list(serving_fn.structured_input_signature[1].keys())}")
        print(f"  è¾“å‡º: {list(serving_fn.structured_outputs.keys())}")

        return bert_model, serving_fn

    except Exception as e:
        print(f"\nâœ— SavedModelåŠ è½½å¤±è´¥: {e}")
        return None, None


def benchmark_tensorflow_savedmodel(serving_fn, test_data, num_warmup, num_test, batch_size):
    """æµ‹è¯•TensorFlow SavedModelæ¨ç†æ€§èƒ½"""
    print(f"\n{'='*70}")
    print("1. TensorFlow SavedModel æ¨ç†æµ‹è¯•")
    print(f"{'='*70}")

    # Warmup
    print(f"\nçƒ­èº«è¿è¡Œ: {num_warmup} iterations...")
    for i in range(num_warmup):
        inputs = {
            'input_word_ids': tf.constant(test_data["input_word_ids"][i:i+batch_size]),
            'input_mask': tf.constant(test_data["input_mask"][i:i+batch_size]),
            'input_type_ids': tf.constant(test_data["input_type_ids"][i:i+batch_size]),
        }
        _ = serving_fn(**inputs)
        if (i + 1) % 5 == 0:
            print(f"  Warmup: {i+1}/{num_warmup}")

    print(f"âœ“ çƒ­èº«å®Œæˆ")

    # æ€§èƒ½æµ‹è¯•
    print(f"\næ€§èƒ½æµ‹è¯•: {num_test} iterations...")
    latencies = []

    for i in range(num_test):
        inputs = {
            'input_word_ids': tf.constant(test_data["input_word_ids"][i:i+batch_size]),
            'input_mask': tf.constant(test_data["input_mask"][i:i+batch_size]),
            'input_type_ids': tf.constant(test_data["input_type_ids"][i:i+batch_size]),
        }

        start = time.perf_counter()
        _ = serving_fn(**inputs)
        end = time.perf_counter()

        latency_ms = (end - start) * 1000
        latencies.append(latency_ms)

        if (i + 1) % 10 == 0:
            print(f"  æµ‹è¯•: {i+1}/{num_test}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    latencies_np = np.array(latencies)

    results = {
        "engine": "TensorFlow SavedModel",
        "latency_mean_ms": float(np.mean(latencies_np)),
        "latency_median_ms": float(np.median(latencies_np)),
        "latency_std_ms": float(np.std(latencies_np)),
        "latency_min_ms": float(np.min(latencies_np)),
        "latency_max_ms": float(np.max(latencies_np)),
        "latency_p50_ms": float(np.percentile(latencies_np, 50)),
        "latency_p95_ms": float(np.percentile(latencies_np, 95)),
        "latency_p99_ms": float(np.percentile(latencies_np, 99)),
        "throughput_samples_per_sec": batch_size * num_test / (np.sum(latencies_np) / 1000),
    }

    print(f"\nâœ“ TensorFlow SavedModel æµ‹è¯•å®Œæˆ!")
    print(f"\nç»“æœ:")
    print(f"  å»¶è¿Ÿ (mean):   {results['latency_mean_ms']:.2f} ms")
    print(f"  å»¶è¿Ÿ (median): {results['latency_median_ms']:.2f} ms")
    print(f"  å»¶è¿Ÿ (p95):    {results['latency_p95_ms']:.2f} ms")
    print(f"  å»¶è¿Ÿ (p99):    {results['latency_p99_ms']:.2f} ms")
    print(f"  ååé‡:        {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def convert_savedmodel_to_onnx(bert_model_path, output_path, seq_length):
    """å°†SavedModelè½¬æ¢ä¸ºONNXæ ¼å¼"""
    print(f"\n{'='*70}")
    print("è½¬æ¢ SavedModel åˆ° ONNX")
    print(f"{'='*70}")

    if not ONNX_AVAILABLE:
        print("\nâœ— ONNX Runtimeæœªå®‰è£…ï¼Œè·³è¿‡è½¬æ¢")
        return None

    try:
        import tf2onnx

        print(f"SavedModelè·¯å¾„: {bert_model_path}")
        print(f"è¾“å‡ºONNXè·¯å¾„: {output_path}")

        # ä½¿ç”¨tf2onnxè½¬æ¢
        print("\nå¼€å§‹è½¬æ¢...")
        print("(å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")

        model_proto, _ = tf2onnx.convert.from_saved_model(
            str(bert_model_path),
            input_names=['input_word_ids:0', 'input_mask:0', 'input_type_ids:0'],
            output_names=None,  # è‡ªåŠ¨æ£€æµ‹
            opset=13,
            extra_opset=None,
        )

        # ä¿å­˜ONNXæ¨¡å‹
        with open(output_path, "wb") as f:
            f.write(model_proto.SerializeToString())

        print(f"âœ“ ONNXæ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
        print(f"  æ¨¡å‹å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")

        return output_path

    except Exception as e:
        print(f"\nâœ— ONNXè½¬æ¢å¤±è´¥: {e}")
        print("  å¯èƒ½åŸå› :")
        print("  1. tf2onnxæœªå®‰è£… (pip install tf2onnx)")
        print("  2. æ¨¡å‹ç»“æ„ä¸å…¼å®¹")
        print("  3. ONNX opsetç‰ˆæœ¬é—®é¢˜")
        return None


def benchmark_onnx(onnx_model_path, test_data, num_warmup, num_test, batch_size):
    """æµ‹è¯•ONNX Runtimeæ¨ç†æ€§èƒ½"""
    print(f"\n{'='*70}")
    print("2. ONNX Runtime æ¨ç†æµ‹è¯•")
    print(f"{'='*70}")

    if not ONNX_AVAILABLE:
        print("\nâœ— ONNX Runtimeæœªå®‰è£…")
        return None

    if not onnx_model_path or not onnx_model_path.exists():
        print(f"\nâœ— ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {onnx_model_path}")
        return None

    try:
        # é…ç½®ONNX Runtime
        session_options = ort.SessionOptions()
        session_options.intra_op_num_threads = os.cpu_count()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        providers = ['CPUExecutionProvider']

        print(f"\nåŠ è½½ONNXæ¨¡å‹: {onnx_model_path}")
        print(f"  çº¿ç¨‹æ•°: {session_options.intra_op_num_threads}")
        print(f"  ä¼˜åŒ–çº§åˆ«: ORT_ENABLE_ALL")
        print(f"  æ‰§è¡Œæä¾›è€…: {providers}")

        session = ort.InferenceSession(
            str(onnx_model_path),
            sess_options=session_options,
            providers=providers
        )

        # è·å–è¾“å…¥/è¾“å‡ºä¿¡æ¯
        input_names = [inp.name for inp in session.get_inputs()]
        output_names = [out.name for out in session.get_outputs()]

        print(f"âœ“ ONNX Runtime ä¼šè¯åˆ›å»ºæˆåŠŸ")
        print(f"  è¾“å…¥: {input_names}")
        print(f"  è¾“å‡ºæ•°é‡: {len(output_names)}")

        # Warmup
        print(f"\nçƒ­èº«è¿è¡Œ: {num_warmup} iterations...")
        for i in range(num_warmup):
            inputs = {
                input_names[0]: test_data["input_word_ids"][i:i+batch_size],
                input_names[1]: test_data["input_mask"][i:i+batch_size],
                input_names[2]: test_data["input_type_ids"][i:i+batch_size],
            }
            _ = session.run(None, inputs)
            if (i + 1) % 5 == 0:
                print(f"  Warmup: {i+1}/{num_warmup}")

        print(f"âœ“ çƒ­èº«å®Œæˆ")

        # æ€§èƒ½æµ‹è¯•
        print(f"\næ€§èƒ½æµ‹è¯•: {num_test} iterations...")
        latencies = []

        for i in range(num_test):
            inputs = {
                input_names[0]: test_data["input_word_ids"][i:i+batch_size],
                input_names[1]: test_data["input_mask"][i:i+batch_size],
                input_names[2]: test_data["input_type_ids"][i:i+batch_size],
            }

            start = time.perf_counter()
            _ = session.run(None, inputs)
            end = time.perf_counter()

            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

            if (i + 1) % 10 == 0:
                print(f"  æµ‹è¯•: {i+1}/{num_test}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        latencies_np = np.array(latencies)

        results = {
            "engine": "ONNX Runtime",
            "latency_mean_ms": float(np.mean(latencies_np)),
            "latency_median_ms": float(np.median(latencies_np)),
            "latency_std_ms": float(np.std(latencies_np)),
            "latency_min_ms": float(np.min(latencies_np)),
            "latency_max_ms": float(np.max(latencies_np)),
            "latency_p50_ms": float(np.percentile(latencies_np, 50)),
            "latency_p95_ms": float(np.percentile(latencies_np, 95)),
            "latency_p99_ms": float(np.percentile(latencies_np, 99)),
            "throughput_samples_per_sec": batch_size * num_test / (np.sum(latencies_np) / 1000),
        }

        print(f"\nâœ“ ONNX Runtime æµ‹è¯•å®Œæˆ!")
        print(f"\nç»“æœ:")
        print(f"  å»¶è¿Ÿ (mean):   {results['latency_mean_ms']:.2f} ms")
        print(f"  å»¶è¿Ÿ (median): {results['latency_median_ms']:.2f} ms")
        print(f"  å»¶è¿Ÿ (p95):    {results['latency_p95_ms']:.2f} ms")
        print(f"  å»¶è¿Ÿ (p99):    {results['latency_p99_ms']:.2f} ms")
        print(f"  ååé‡:        {results['throughput_samples_per_sec']:.2f} samples/sec")

        return results

    except Exception as e:
        print(f"\nâœ— ONNX Runtimeæµ‹è¯•å¤±è´¥: {e}")
        return None


def generate_comparison_report(tf_results, onnx_results, output_dir, config):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print(f"\n{'='*70}")
    print("ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
    print(f"{'='*70}")

    report_file = output_dir / "comparison_report.md"

    with open(report_file, "w") as f:
        f.write("# BERT CPUæ¨ç†æ€§èƒ½å¯¹æ¯”: TensorFlow vs ONNX Runtime\n\n")
        f.write("**æµ‹è¯•æ–¹æ³•**: ä½¿ç”¨SavedModelç›´æ¥åŠ è½½ï¼Œé¿å…KerasLayeré—®é¢˜\n\n")

        f.write("## æµ‹è¯•é…ç½®\n\n")
        f.write(f"- **æ¨¡å‹**: BERT-base (TensorFlow Hub SavedModel)\n")
        f.write(f"- **Batch Size**: {config['batch_size']}\n")
        f.write(f"- **åºåˆ—é•¿åº¦**: {config['seq_length']}\n")
        f.write(f"- **çƒ­èº«è¿­ä»£**: {config['num_warmup']}\n")
        f.write(f"- **æµ‹è¯•è¿­ä»£**: {config['num_test']}\n")
        f.write(f"- **TensorFlow ç‰ˆæœ¬**: {tf.__version__}\n")
        if ONNX_AVAILABLE:
            f.write(f"- **ONNX Runtime ç‰ˆæœ¬**: {ort.__version__}\n")
        f.write("\n")

        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")

        if onnx_results:
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup_mean = tf_results['latency_mean_ms'] / onnx_results['latency_mean_ms']
            speedup_p95 = tf_results['latency_p95_ms'] / onnx_results['latency_p95_ms']
            speedup_throughput = onnx_results['throughput_samples_per_sec'] / tf_results['throughput_samples_per_sec']

            f.write("| æŒ‡æ ‡ | TensorFlow | ONNX Runtime | åŠ é€Ÿæ¯” |\n")
            f.write("|------|------------|--------------|--------|\n")
            f.write(f"| å»¶è¿Ÿ (mean) | {tf_results['latency_mean_ms']:.2f} ms | {onnx_results['latency_mean_ms']:.2f} ms | {speedup_mean:.2f}x |\n")
            f.write(f"| å»¶è¿Ÿ (median) | {tf_results['latency_median_ms']:.2f} ms | {onnx_results['latency_median_ms']:.2f} ms | {tf_results['latency_median_ms']/onnx_results['latency_median_ms']:.2f}x |\n")
            f.write(f"| å»¶è¿Ÿ (std) | {tf_results['latency_std_ms']:.2f} ms | {onnx_results['latency_std_ms']:.2f} ms | - |\n")
            f.write(f"| å»¶è¿Ÿ (min) | {tf_results['latency_min_ms']:.2f} ms | {onnx_results['latency_min_ms']:.2f} ms | - |\n")
            f.write(f"| å»¶è¿Ÿ (max) | {tf_results['latency_max_ms']:.2f} ms | {onnx_results['latency_max_ms']:.2f} ms | - |\n")
            f.write(f"| å»¶è¿Ÿ (p95) | {tf_results['latency_p95_ms']:.2f} ms | {onnx_results['latency_p95_ms']:.2f} ms | {speedup_p95:.2f}x |\n")
            f.write(f"| å»¶è¿Ÿ (p99) | {tf_results['latency_p99_ms']:.2f} ms | {onnx_results['latency_p99_ms']:.2f} ms | {tf_results['latency_p99_ms']/onnx_results['latency_p99_ms']:.2f}x |\n")
            f.write(f"| ååé‡ | {tf_results['throughput_samples_per_sec']:.2f} samples/s | {onnx_results['throughput_samples_per_sec']:.2f} samples/s | {speedup_throughput:.2f}x |\n\n")

            f.write("## æ€»ç»“\n\n")
            if speedup_mean > 1.0:
                f.write(f"âœ… **ONNX Runtime æ¯” TensorFlow å¿« {speedup_mean:.2f}x**\n\n")
            else:
                f.write(f"âœ… **TensorFlow æ¯” ONNX Runtime å¿« {1/speedup_mean:.2f}x**\n\n")

            f.write(f"- **å¹³å‡å»¶è¿Ÿæå‡**: {speedup_mean:.2f}x\n")
            f.write(f"- **P95å»¶è¿Ÿæå‡**: {speedup_p95:.2f}x\n")
            f.write(f"- **ååé‡æå‡**: {speedup_throughput:.2f}x\n\n")

            # æ€§èƒ½åˆ†æ
            f.write("### æ€§èƒ½åˆ†æ\n\n")
            if speedup_mean >= 1.5:
                f.write(f"ğŸš€ ONNX Runtimeæ˜¾è‘—ä¼˜äºTensorFlowï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€‚\n\n")
            elif speedup_mean >= 1.1:
                f.write(f"âœ… ONNX Runtimeæ€§èƒ½ä¼˜äºTensorFlowï¼Œé€‚åˆå¯¹å»¶è¿Ÿæ•æ„Ÿçš„åœºæ™¯ã€‚\n\n")
            elif speedup_mean >= 0.9:
                f.write(f"âš–ï¸ ä¸¤ä¸ªå¼•æ“æ€§èƒ½ç›¸å½“ï¼Œå¯æ ¹æ®å…¶ä»–å› ç´ é€‰æ‹©ã€‚\n\n")
            else:
                f.write(f"âš ï¸ TensorFlowåœ¨æ­¤é…ç½®ä¸‹æ€§èƒ½æ›´å¥½ã€‚\n\n")

        else:
            # ä»…æœ‰TensorFlowç»“æœ
            f.write("| æŒ‡æ ‡ | TensorFlow | ONNX Runtime |\n")
            f.write("|------|------------|-------------|\n")
            f.write(f"| å»¶è¿Ÿ (mean) | {tf_results['latency_mean_ms']:.2f} ms | N/A |\n")
            f.write(f"| å»¶è¿Ÿ (median) | {tf_results['latency_median_ms']:.2f} ms | N/A |\n")
            f.write(f"| å»¶è¿Ÿ (p95) | {tf_results['latency_p95_ms']:.2f} ms | N/A |\n")
            f.write(f"| å»¶è¿Ÿ (p99) | {tf_results['latency_p99_ms']:.2f} ms | N/A |\n")
            f.write(f"| ååé‡ | {tf_results['throughput_samples_per_sec']:.2f} samples/s | N/A |\n\n")

            f.write("## è¯´æ˜\n\n")
            f.write("âš ï¸ ONNXæ¨¡å‹è½¬æ¢æˆ–æµ‹è¯•å¤±è´¥ï¼Œä»…æ˜¾ç¤ºTensorFlowç»“æœã€‚\n\n")

        f.write("## æŠ€æœ¯è¯´æ˜\n\n")
        f.write("### SavedModelåŠ è½½æ–¹å¼\n\n")
        f.write("æœ¬æµ‹è¯•ä½¿ç”¨SavedModelæ–¹å¼ç›´æ¥åŠ è½½BERTæ¨¡å‹ï¼ŒæˆåŠŸé¿å…äº†TensorFlow Hub ")
        f.write("KerasLayeråœ¨TF 2.20ä¸­çš„KerasTensorå…¼å®¹æ€§é—®é¢˜ã€‚\n\n")

        f.write("**å¯¹æ¯”**:\n")
        f.write("- âŒ **åŸå§‹æ–¹æ³•** (å¤±è´¥): `hub.KerasLayer()` â†’ KerasTensorè½¬æ¢é”™è¯¯\n")
        f.write("- âœ… **æ–°æ–¹æ³•** (æˆåŠŸ): `tf.saved_model.load()` â†’ ç›´æ¥ä½¿ç”¨serving signature\n\n")

        if onnx_results:
            f.write("### ONNXè½¬æ¢\n\n")
            f.write("- ä½¿ç”¨ `tf2onnx` å°†SavedModelè½¬æ¢ä¸ºONNXæ ¼å¼\n")
            f.write("- ONNX opset ç‰ˆæœ¬: 13\n")
            f.write("- ä¿ç•™å®Œæ•´çš„BERTæ¨¡å‹ç»“æ„\n\n")

        f.write("## ç¯å¢ƒä¿¡æ¯\n\n")
        f.write(f"- Python: {'.'.join(map(str, __import__('sys').version_info[:3]))}\n")
        f.write(f"- TensorFlow: {tf.__version__}\n")
        f.write(f"- NumPy: {np.__version__}\n")
        if ONNX_AVAILABLE:
            f.write(f"- ONNX Runtime: {ort.__version__}\n")

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def main():
    args = parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    models_dir = output_dir / "models"
    models_dir.mkdir(exist_ok=True)

    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(
        num_samples=args.num_test,
        seq_length=args.seq_length,
        batch_size=args.batch_size
    )

    # åŠ è½½BERTæ¨¡å‹ (SavedModelæ–¹å¼)
    print(f"\n{'='*70}")
    print("åŠ è½½ BERT æ¨¡å‹ (SavedModel)")
    print(f"{'='*70}")

    bert_model, serving_fn = load_bert_with_savedmodel(models_dir, args.seq_length)

    if serving_fn is None:
        print("\nâœ— BERTæ¨¡å‹åŠ è½½å¤±è´¥")
        print("å¯èƒ½çš„åŸå› :")
        print("  1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("  2. TensorFlow Hubä¸‹è½½å¤±è´¥")
        print("  3. SavedModelæ ¼å¼ä¸å…¼å®¹")
        return

    # æµ‹è¯• TensorFlow SavedModel
    tf_results = benchmark_tensorflow_savedmodel(
        serving_fn=serving_fn,
        test_data=test_data,
        num_warmup=args.num_warmup,
        num_test=args.num_test,
        batch_size=args.batch_size
    )

    # ä¿å­˜TensorFlowç»“æœ
    tf_result_file = output_dir / "tensorflow_savedmodel_results.json"
    with open(tf_result_file, "w") as f:
        json.dump(tf_results, f, indent=2)
    print(f"\nâœ“ TensorFlowç»“æœå·²ä¿å­˜åˆ°: {tf_result_file}")

    # è½¬æ¢å¹¶æµ‹è¯• ONNX
    onnx_results = None
    onnx_result_file = None

    if ONNX_AVAILABLE:
        # è·å–SavedModelè·¯å¾„
        model_path = Path(hub.resolve(
            "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"
        ))

        # è½¬æ¢ä¸ºONNX
        onnx_model_path = output_dir / "bert_model.onnx"
        if not onnx_model_path.exists():
            onnx_model_path = convert_savedmodel_to_onnx(
                model_path,
                onnx_model_path,
                args.seq_length
            )
        else:
            print(f"\nâœ“ ONNXæ¨¡å‹å·²å­˜åœ¨: {onnx_model_path}")

        # æµ‹è¯•ONNX Runtime
        if onnx_model_path and onnx_model_path.exists():
            onnx_results = benchmark_onnx(
                onnx_model_path=onnx_model_path,
                test_data=test_data,
                num_warmup=args.num_warmup,
                num_test=args.num_test,
                batch_size=args.batch_size
            )

            if onnx_results:
                onnx_result_file = output_dir / "onnx_runtime_results.json"
                with open(onnx_result_file, "w") as f:
                    json.dump(onnx_results, f, indent=2)
                print(f"\nâœ“ ONNXç»“æœå·²ä¿å­˜åˆ°: {onnx_result_file}")
    else:
        print(f"\nâš ï¸ ONNX Runtimeæœªå®‰è£…ï¼Œè·³è¿‡ONNXæµ‹è¯•")
        print(f"   å®‰è£…æ–¹æ³•: pip install onnxruntime tf2onnx")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    config = {
        "batch_size": args.batch_size,
        "seq_length": args.seq_length,
        "num_warmup": args.num_warmup,
        "num_test": args.num_test,
    }

    report_file = generate_comparison_report(tf_results, onnx_results, output_dir, config)

    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*70}")
    print("âœ“ BERT æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print(f"{'='*70}")
    print(f"\nç»“æœæ–‡ä»¶:")
    print(f"  - TensorFlow SavedModel: {tf_result_file}")
    if onnx_result_file:
        print(f"  - ONNX Runtime: {onnx_result_file}")
    print(f"  - å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    print(f"\nè¯´æ˜:")
    print(f"  âœ… æˆåŠŸä½¿ç”¨SavedModelæ–¹å¼åŠ è½½BERT")
    print(f"  âœ… é¿å…äº†KerasLayerçš„KerasTensoré—®é¢˜")
    if onnx_results:
        speedup = tf_results['latency_mean_ms'] / onnx_results['latency_mean_ms']
        if speedup > 1.0:
            print(f"  ğŸš€ ONNX Runtime æ¯” TensorFlow å¿« {speedup:.2f}x")
        else:
            print(f"  â„¹ï¸ TensorFlow æ¯” ONNX Runtime å¿« {1/speedup:.2f}x")
    else:
        print(f"  âš ï¸ ONNXæµ‹è¯•æœªè¿è¡Œ")
    print(f"\n{'='*70}")


if __name__ == "__main__":
    main()
