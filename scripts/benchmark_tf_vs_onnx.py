#!/usr/bin/env python3
"""
TensorFlow vs ONNX æ€§èƒ½å¯¹æ¯”æµ‹è¯•

å¯¹æ¯”ç›¸åŒæ¨¡å‹åœ¨TensorFlowå’ŒONNX Runtimeä¸‹çš„æ¨ç†æ€§èƒ½
è§£å†³ TODO.md Issue #3
"""

import os
import sys
import time
import json
import argparse
import subprocess
from pathlib import Path
import numpy as np

def print_section(title):
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print_section("ç¯å¢ƒæ£€æŸ¥")

    env_info = {}

    try:
        import tensorflow as tf
        env_info['tensorflow'] = tf.__version__
        print(f"âœ“ TensorFlow: {tf.__version__}")
    except Exception as e:
        print(f"âœ— TensorFlow: {e}")
        sys.exit(1)

    try:
        import numpy as np
        env_info['numpy'] = np.__version__
        print(f"âœ“ NumPy: {np.__version__}")
    except Exception as e:
        print(f"âœ— NumPy: {e}")

    try:
        import tf2onnx
        env_info['tf2onnx'] = tf2onnx.__version__
        print(f"âœ“ tf2onnx: {tf2onnx.__version__}")
    except Exception as e:
        print(f"âœ— tf2onnx: {e}")
        sys.exit(1)

    try:
        import onnxruntime as ort
        env_info['onnxruntime'] = ort.__version__
        print(f"âœ“ ONNXRuntime: {ort.__version__}")
    except Exception as e:
        print(f"âœ— ONNXRuntime: {e}")
        sys.exit(1)

    try:
        import google.protobuf
        env_info['protobuf'] = google.protobuf.__version__
        print(f"âœ“ Protobuf: {google.protobuf.__version__}")
    except Exception as e:
        print(f"âœ— Protobuf: {e}")

    return env_info


def create_test_model(model_type="cnn"):
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    import tensorflow as tf

    if model_type == "cnn":
        print("åˆ›å»ºCNNæ¨¡å‹...")
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(32, 3, activation='relu'),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Conv2D(64, 3, activation='relu'),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(10, activation='softmax')
        ])
        input_shape = (1, 28, 28, 1)

    elif model_type == "dense":
        print("åˆ›å»ºDenseæ¨¡å‹...")
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(784,)),
            tf.keras.layers.Dense(512, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(10, activation='softmax')
        ])
        input_shape = (1, 784)

    else:
        raise ValueError(f"Unknown model type: {model_type}")

    print(f"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ€»æ•°: {model.count_params():,}")
    return model, input_shape


def benchmark_tensorflow(model, test_data, num_runs=100, num_warmup=10):
    """æµ‹è¯•TensorFlowæ¨¡å‹æ€§èƒ½"""
    print_section("TensorFlow æ€§èƒ½æµ‹è¯•")

    # çƒ­èº«
    print(f"çƒ­èº«: {num_warmup} iterations...")
    for _ in range(num_warmup):
        _ = model(test_data, training=False)

    # æ€§èƒ½æµ‹è¯•
    print(f"æ€§èƒ½æµ‹è¯•: {num_runs} iterations...")
    latencies = []

    for i in range(num_runs):
        start = time.perf_counter()
        _ = model(test_data, training=False)
        latency = (time.perf_counter() - start) * 1000  # ms
        latencies.append(latency)

        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i+1}/{num_runs}")

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    results = {
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "std_ms": float(np.std(latencies)),
        "p50_ms": float(np.percentile(latencies, 50)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": float(1000.0 / np.mean(latencies))
    }

    print("\nâœ“ TensorFlow æµ‹è¯•å®Œæˆ")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def convert_to_onnx(model, output_path):
    """å°†TensorFlowæ¨¡å‹è½¬æ¢ä¸ºONNX"""
    print_section("è½¬æ¢ä¸ºONNX")

    import tensorflow as tf

    # å…ˆä¿å­˜ä¸ºSavedModel
    saved_model_path = Path(output_path).parent / "temp_savedmodel"
    model.export(saved_model_path)
    print(f"âœ“ SavedModelå·²ä¿å­˜: {saved_model_path}")

    # è½¬æ¢ä¸ºONNX
    print("è½¬æ¢ä¸ºONNX...")
    start_time = time.time()

    cmd = [
        "python3", "-m", "tf2onnx.convert",
        "--saved-model", str(saved_model_path),
        "--output", str(output_path),
        "--opset", "13"
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âœ— ONNXè½¬æ¢å¤±è´¥")
        print(f"é”™è¯¯: {result.stderr}")
        return None

    conversion_time = time.time() - start_time
    file_size = os.path.getsize(output_path) / (1024 * 1024)

    print(f"âœ“ ONNXè½¬æ¢æˆåŠŸ")
    print(f"  è½¬æ¢æ—¶é—´: {conversion_time:.2f}s")
    print(f"  æ¨¡å‹å¤§å°: {file_size:.2f} MB")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")

    return {
        "conversion_time": conversion_time,
        "model_size_mb": file_size
    }


def benchmark_onnx(onnx_path, test_data, num_runs=100, num_warmup=10):
    """æµ‹è¯•ONNXæ¨¡å‹æ€§èƒ½"""
    print_section("ONNX Runtime æ€§èƒ½æµ‹è¯•")

    import onnxruntime as ort

    # åˆ›å»ºä¼šè¯
    sess = ort.InferenceSession(str(onnx_path))

    input_name = sess.get_inputs()[0].name
    output_name = sess.get_outputs()[0].name

    print(f"  è¾“å…¥åç§°: {input_name}")
    print(f"  è¾“å‡ºåç§°: {output_name}")

    # çƒ­èº«
    print(f"\nçƒ­èº«: {num_warmup} iterations...")
    for _ in range(num_warmup):
        _ = sess.run([output_name], {input_name: test_data})

    # æ€§èƒ½æµ‹è¯•
    print(f"æ€§èƒ½æµ‹è¯•: {num_runs} iterations...")
    latencies = []

    for i in range(num_runs):
        start = time.perf_counter()
        _ = sess.run([output_name], {input_name: test_data})
        latency = (time.perf_counter() - start) * 1000  # ms
        latencies.append(latency)

        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i+1}/{num_runs}")

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    results = {
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "std_ms": float(np.std(latencies)),
        "p50_ms": float(np.percentile(latencies, 50)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": float(1000.0 / np.mean(latencies))
    }

    print("\nâœ“ ONNX Runtime æµ‹è¯•å®Œæˆ")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def generate_report(env_info, tf_results, onnx_results, conversion_info, output_dir):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report_path = Path(output_dir) / "tf_vs_onnx_report.md"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯”\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## ç¯å¢ƒä¿¡æ¯\n\n")
        for key, value in env_info.items():
            f.write(f"- {key}: {value}\n")

        f.write("\n## ONNXè½¬æ¢ä¿¡æ¯\n\n")
        if conversion_info:
            f.write(f"- è½¬æ¢æ—¶é—´: {conversion_info['conversion_time']:.2f}s\n")
            f.write(f"- æ¨¡å‹å¤§å°: {conversion_info['model_size_mb']:.2f} MB\n")

        f.write("\n## å»¶è¿Ÿå¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | TensorFlow | ONNX Runtime | æ¯”ç‡ |\n")
        f.write("|------|-----------|--------------|------|\n")

        speedup_mean = tf_results['mean_ms'] / onnx_results['mean_ms']
        speedup_p95 = tf_results['p95_ms'] / onnx_results['p95_ms']

        f.write(f"| å¹³å‡å»¶è¿Ÿ | {tf_results['mean_ms']:.2f} ms | ")
        f.write(f"{onnx_results['mean_ms']:.2f} ms | ")
        f.write(f"{speedup_mean:.2f}x {'ğŸš€' if speedup_mean > 1 else ''} |\n")

        f.write(f"| ä¸­ä½å»¶è¿Ÿ | {tf_results['median_ms']:.2f} ms | ")
        f.write(f"{onnx_results['median_ms']:.2f} ms | ")
        speedup_median = tf_results['median_ms'] / onnx_results['median_ms']
        f.write(f"{speedup_median:.2f}x |\n")

        f.write(f"| P95å»¶è¿Ÿ | {tf_results['p95_ms']:.2f} ms | ")
        f.write(f"{onnx_results['p95_ms']:.2f} ms | ")
        f.write(f"{speedup_p95:.2f}x |\n")

        f.write(f"| P99å»¶è¿Ÿ | {tf_results['p99_ms']:.2f} ms | ")
        f.write(f"{onnx_results['p99_ms']:.2f} ms | ")
        speedup_p99 = tf_results['p99_ms'] / onnx_results['p99_ms']
        f.write(f"{speedup_p99:.2f}x |\n")

        f.write("\n## ååé‡å¯¹æ¯”\n\n")
        f.write("| æ¡†æ¶ | ååé‡ (samples/s) |\n")
        f.write("|------|-------------------|\n")
        f.write(f"| TensorFlow | {tf_results['throughput_samples_per_sec']:.2f} |\n")
        f.write(f"| ONNX Runtime | {onnx_results['throughput_samples_per_sec']:.2f} |\n")

        throughput_speedup = onnx_results['throughput_samples_per_sec'] / tf_results['throughput_samples_per_sec']
        f.write(f"\n**ååé‡æå‡**: {throughput_speedup:.2f}x\n")

        f.write("\n## æ€»ç»“\n\n")
        if speedup_mean > 1:
            f.write(f"âœ… **ONNX Runtime å¹³å‡å»¶è¿Ÿæ›´ä½ï¼Œæé€Ÿ {speedup_mean:.2f}x**\n\n")
        else:
            f.write(f"âš ï¸ TensorFlow åœ¨æ­¤æ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½\n\n")

        f.write("### å…³é”®å‘ç°\n\n")
        f.write(f"- å¹³å‡å»¶è¿Ÿæå‡: {speedup_mean:.2f}x\n")
        f.write(f"- P95å»¶è¿Ÿæå‡: {speedup_p95:.2f}x\n")
        f.write(f"- ååé‡æå‡: {throughput_speedup:.2f}x\n")

        f.write("\n### å»ºè®®\n\n")
        if speedup_mean > 1.5:
            f.write("âœ… **æ¨èä½¿ç”¨ONNX Runtimeè¿›è¡Œç”Ÿäº§éƒ¨ç½²**\n\n")
            f.write("ONNX Runtimeåœ¨æ­¤æ¨¡å‹ä¸Šæœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿\n")
        elif speedup_mean > 1.1:
            f.write("âœ… **ONNX Runtimeæœ‰ä¸€å®šä¼˜åŠ¿**\n\n")
            f.write("å»ºè®®æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨ç†å¼•æ“\n")
        else:
            f.write("âš ï¸ **æ€§èƒ½å·®å¼‚ä¸æ˜æ˜¾**\n\n")
            f.write("å¯ä»¥æ ¹æ®éƒ¨ç½²ä¾¿åˆ©æ€§å’Œç”Ÿæ€ç³»ç»Ÿé€‰æ‹©æ¨ç†å¼•æ“\n")

    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return str(report_path)


def main():
    parser = argparse.ArgumentParser(description="TensorFlow vs ONNXæ€§èƒ½å¯¹æ¯”")
    parser.add_argument("--model-type", default="cnn", choices=["cnn", "dense"],
                       help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--output-dir", default="results/tf_vs_onnx_benchmark",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num-runs", type=int, default=100,
                       help="æ€§èƒ½æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num-warmup", type=int, default=10,
                       help="çƒ­èº«è¿­ä»£æ¬¡æ•°")
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç¯å¢ƒæ£€æŸ¥
    env_info = check_environment()

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    print_section("åˆ›å»ºæµ‹è¯•æ¨¡å‹")
    import tensorflow as tf
    model, input_shape = create_test_model(args.model_type)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nåˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_data = np.random.randn(*input_shape).astype(np.float32)
    print(f"âœ“ æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")

    # æµ‹è¯•TensorFlowæ€§èƒ½
    tf_results = benchmark_tensorflow(
        model, test_data,
        num_runs=args.num_runs,
        num_warmup=args.num_warmup
    )

    # è½¬æ¢ä¸ºONNX
    onnx_path = output_dir / "model.onnx"
    conversion_info = convert_to_onnx(model, onnx_path)

    if not conversion_info:
        print("âœ— ONNXè½¬æ¢å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
        return

    # æµ‹è¯•ONNXæ€§èƒ½
    onnx_results = benchmark_onnx(
        onnx_path, test_data,
        num_runs=args.num_runs,
        num_warmup=args.num_warmup
    )

    # ä¿å­˜ç»“æœ
    results = {
        "environment": env_info,
        "tensorflow": tf_results,
        "onnx": onnx_results,
        "conversion": conversion_info
    }

    results_json = output_dir / "results.json"
    with open(results_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {results_json}")

    # ç”ŸæˆæŠ¥å‘Š
    report_path = generate_report(env_info, tf_results, onnx_results, conversion_info, output_dir)

    # æ‰“å°æ€»ç»“
    print_section("âœ“ æµ‹è¯•å®Œæˆ!")
    print(f"\nç»“æœæ–‡ä»¶:")
    print(f"  - JSONç»“æœ: {results_json}")
    print(f"  - å¯¹æ¯”æŠ¥å‘Š: {report_path}")

    speedup = tf_results['mean_ms'] / onnx_results['mean_ms']
    print(f"\næ€§èƒ½æå‡: {speedup:.2f}x")

    if speedup > 1:
        print(f"âœ… ONNX Runtime æ¯” TensorFlow å¿« {speedup:.2f}x")
    else:
        print(f"âš ï¸ TensorFlow åœ¨æ­¤æ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½")


if __name__ == "__main__":
    main()
