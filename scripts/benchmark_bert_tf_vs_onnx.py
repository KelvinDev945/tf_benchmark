#!/usr/bin/env python3
"""
BERTæ¨¡å‹ TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯”æµ‹è¯•

ä½¿ç”¨BERTæ¶æ„æµ‹è¯•ONNXè½¬æ¢å’Œæ€§èƒ½æå‡
è§£å†³ TODO.md Issue #3
"""

import os
import sys
import time
import json
import argparse
import subprocess
from pathlib import Path
import numpy as np
import tensorflow as tf

def print_section(title):
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print_section("ç¯å¢ƒæ£€æŸ¥")

    env_info = {}

    env_info['tensorflow'] = tf.__version__
    print(f"âœ“ TensorFlow: {tf.__version__}")

    import numpy as np
    env_info['numpy'] = np.__version__
    print(f"âœ“ NumPy: {np.__version__}")

    try:
        import tf2onnx
        env_info['tf2onnx'] = tf2onnx.__version__
        print(f"âœ“ tf2onnx: {tf2onnx.__version__}")
    except Exception as e:
        print(f"âœ— tf2onnx: {e}")
        sys.exit(1)

    try:
        import onnxruntime as ort
        env_info['onnxruntime'] = ort.__version__
        print(f"âœ“ ONNXRuntime: {ort.__version__}")
    except Exception as e:
        print(f"âœ— ONNXRuntime: {e}")
        sys.exit(1)

    try:
        import google.protobuf
        env_info['protobuf'] = google.protobuf.__version__
        print(f"âœ“ Protobuf: {google.protobuf.__version__}")
    except Exception as e:
        print(f"âœ— Protobuf: {e}")

    return env_info


def create_bert_model(seq_length=128, vocab_size=30522, hidden_size=768,
                      num_hidden_layers=12, num_attention_heads=12,
                      intermediate_size=3072):
    """
    åˆ›å»ºBERT-Baseæ¶æ„æ¨¡å‹

    å‚æ•°:
        seq_length: åºåˆ—é•¿åº¦
        vocab_size: è¯æ±‡è¡¨å¤§å°
        hidden_size: éšè—å±‚å¤§å°
        num_hidden_layers: Transformerå±‚æ•°
        num_attention_heads: æ³¨æ„åŠ›å¤´æ•°
        intermediate_size: FFNä¸­é—´å±‚å¤§å°
    """
    print_section("åˆ›å»ºBERTæ¨¡å‹")
    print(f"é…ç½®:")
    print(f"  åºåˆ—é•¿åº¦: {seq_length}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  éšè—å±‚å¤§å°: {hidden_size}")
    print(f"  Transformerå±‚æ•°: {num_hidden_layers}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_attention_heads}")

    # è¾“å…¥å±‚
    input_ids = tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32, name='input_ids')

    # Embeddingå±‚
    embeddings = tf.keras.layers.Embedding(
        vocab_size, hidden_size, name='embedding'
    )(input_ids)

    # Position Embedding
    position_embeddings = tf.keras.layers.Embedding(
        seq_length, hidden_size, name='position_embedding'
    )(tf.range(seq_length))

    # åˆå¹¶embeddings
    x = embeddings + position_embeddings
    x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x)
    x = tf.keras.layers.Dropout(0.1)(x)

    # Transformer Encoder å±‚
    for i in range(num_hidden_layers):
        # Multi-Head Attention
        attention_output = tf.keras.layers.MultiHeadAttention(
            num_heads=num_attention_heads,
            key_dim=hidden_size // num_attention_heads,
            name=f'attention_{i}'
        )(x, x)

        attention_output = tf.keras.layers.Dropout(0.1)(attention_output)
        x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x + attention_output)

        # Feed Forward Network
        # ä½¿ç”¨reluæ›¿ä»£geluä»¥å…¼å®¹ONNX Runtime
        ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(intermediate_size, activation='relu'),
            tf.keras.layers.Dense(hidden_size)
        ], name=f'ffn_{i}')

        ffn_output = ffn(x)
        ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)
        x = tf.keras.layers.LayerNormalization(epsilon=1e-12)(x + ffn_output)

    # Pooler
    pooled_output = tf.keras.layers.Lambda(lambda x: x[:, 0])(x)
    pooled_output = tf.keras.layers.Dense(
        hidden_size, activation='tanh', name='pooler'
    )(pooled_output)

    # åˆ†ç±»å¤´ (ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡)
    output = tf.keras.layers.Dense(2, activation='softmax', name='classifier')(pooled_output)

    model = tf.keras.Model(inputs=input_ids, outputs=output, name='bert_model')

    print(f"\nâœ“ BERTæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"  æ€»å‚æ•°: {model.count_params():,}")

    # æ˜¾ç¤ºæ¨¡å‹å¤§å°ä¼°è®¡
    param_size_mb = model.count_params() * 4 / (1024 * 1024)  # å‡è®¾float32
    print(f"  ä¼°è®¡å¤§å°: {param_size_mb:.2f} MB")

    return model


def create_bert_lite_model(seq_length=128, vocab_size=10000, hidden_size=256,
                            num_hidden_layers=4, num_attention_heads=4):
    """åˆ›å»ºè½»é‡çº§BERTæ¨¡å‹ï¼ˆæ›´å¿«æµ‹è¯•ï¼‰"""
    return create_bert_model(
        seq_length=seq_length,
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        intermediate_size=hidden_size * 4
    )


def benchmark_tensorflow(model, test_data, num_runs=100, num_warmup=10):
    """æµ‹è¯•TensorFlowæ¨¡å‹æ€§èƒ½"""
    print_section("TensorFlow æ€§èƒ½æµ‹è¯•")

    # çƒ­èº«
    print(f"çƒ­èº«: {num_warmup} iterations...")
    for i in range(num_warmup):
        _ = model(test_data, training=False)
        if (i + 1) % 5 == 0:
            print(f"  çƒ­èº«: {i+1}/{num_warmup}")

    # æ€§èƒ½æµ‹è¯•
    print(f"\næ€§èƒ½æµ‹è¯•: {num_runs} iterations...")
    latencies = []

    for i in range(num_runs):
        start = time.perf_counter()
        _ = model(test_data, training=False)
        latency = (time.perf_counter() - start) * 1000  # ms
        latencies.append(latency)

        if (i + 1) % 20 == 0:
            print(f"  æµ‹è¯•: {i+1}/{num_runs}")

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    results = {
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "std_ms": float(np.std(latencies)),
        "p50_ms": float(np.percentile(latencies, 50)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": float(1000.0 / np.mean(latencies))
    }

    print("\nâœ“ TensorFlow æµ‹è¯•å®Œæˆ")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def convert_to_onnx(model, output_path):
    """å°†TensorFlowæ¨¡å‹è½¬æ¢ä¸ºONNX"""
    print_section("è½¬æ¢ä¸ºONNX")

    # å…ˆä¿å­˜ä¸ºSavedModel
    saved_model_path = Path(output_path).parent / "temp_savedmodel_bert"
    print("ä¿å­˜ä¸ºSavedModel...")
    model.export(saved_model_path)
    print(f"âœ“ SavedModelå·²ä¿å­˜: {saved_model_path}")

    # è½¬æ¢ä¸ºONNX
    print("\nè½¬æ¢ä¸ºONNX...")
    start_time = time.time()

    cmd = [
        "python3", "-m", "tf2onnx.convert",
        "--saved-model", str(saved_model_path),
        "--output", str(output_path),
        "--opset", "13"
    ]

    print("æ‰§è¡Œè½¬æ¢å‘½ä»¤...")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âœ— ONNXè½¬æ¢å¤±è´¥")
        print(f"é”™è¯¯: {result.stderr}")
        return None

    conversion_time = time.time() - start_time
    file_size = os.path.getsize(output_path) / (1024 * 1024)

    print(f"\nâœ“ ONNXè½¬æ¢æˆåŠŸ")
    print(f"  è½¬æ¢æ—¶é—´: {conversion_time:.2f}s")
    print(f"  æ¨¡å‹å¤§å°: {file_size:.2f} MB")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")

    return {
        "conversion_time": conversion_time,
        "model_size_mb": file_size
    }


def benchmark_onnx(onnx_path, test_data, num_runs=100, num_warmup=10):
    """æµ‹è¯•ONNXæ¨¡å‹æ€§èƒ½"""
    print_section("ONNX Runtime æ€§èƒ½æµ‹è¯•")

    import onnxruntime as ort

    # åˆ›å»ºä¼šè¯
    print("åˆ›å»ºONNX Runtimeä¼šè¯...")
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    sess = ort.InferenceSession(str(onnx_path), sess_options)

    input_name = sess.get_inputs()[0].name
    output_name = sess.get_outputs()[0].name

    print(f"  è¾“å…¥åç§°: {input_name}")
    print(f"  è¾“å‡ºåç§°: {output_name}")
    print(f"  è¾“å…¥å½¢çŠ¶: {sess.get_inputs()[0].shape}")

    # çƒ­èº«
    print(f"\nçƒ­èº«: {num_warmup} iterations...")
    for i in range(num_warmup):
        _ = sess.run([output_name], {input_name: test_data})
        if (i + 1) % 5 == 0:
            print(f"  çƒ­èº«: {i+1}/{num_warmup}")

    # æ€§èƒ½æµ‹è¯•
    print(f"\næ€§èƒ½æµ‹è¯•: {num_runs} iterations...")
    latencies = []

    for i in range(num_runs):
        start = time.perf_counter()
        _ = sess.run([output_name], {input_name: test_data})
        latency = (time.perf_counter() - start) * 1000  # ms
        latencies.append(latency)

        if (i + 1) % 20 == 0:
            print(f"  æµ‹è¯•: {i+1}/{num_runs}")

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    results = {
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "std_ms": float(np.std(latencies)),
        "p50_ms": float(np.percentile(latencies, 50)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": float(1000.0 / np.mean(latencies))
    }

    print("\nâœ“ ONNX Runtime æµ‹è¯•å®Œæˆ")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def generate_report(env_info, model_config, tf_results, onnx_results, conversion_info, output_dir):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report_path = Path(output_dir) / "bert_tf_vs_onnx_report.md"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# BERTæ¨¡å‹ TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯”\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## ç¯å¢ƒä¿¡æ¯\n\n")
        for key, value in env_info.items():
            f.write(f"- {key}: {value}\n")

        f.write("\n## BERTæ¨¡å‹é…ç½®\n\n")
        for key, value in model_config.items():
            f.write(f"- {key}: {value}\n")

        f.write("\n## ONNXè½¬æ¢ä¿¡æ¯\n\n")
        if conversion_info:
            f.write(f"- è½¬æ¢æ—¶é—´: {conversion_info['conversion_time']:.2f}s\n")
            f.write(f"- æ¨¡å‹å¤§å°: {conversion_info['model_size_mb']:.2f} MB\n")

        f.write("\n## å»¶è¿Ÿå¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | TensorFlow | ONNX Runtime | æå‡å€æ•° |\n")
        f.write("|------|-----------|--------------|----------|\n")

        speedup_mean = tf_results['mean_ms'] / onnx_results['mean_ms']
        speedup_p95 = tf_results['p95_ms'] / onnx_results['p95_ms']

        f.write(f"| å¹³å‡å»¶è¿Ÿ | {tf_results['mean_ms']:.2f} ms | ")
        f.write(f"{onnx_results['mean_ms']:.2f} ms | ")
        f.write(f"{speedup_mean:.2f}x {'ğŸš€' if speedup_mean > 1 else ''} |\n")

        f.write(f"| ä¸­ä½å»¶è¿Ÿ | {tf_results['median_ms']:.2f} ms | ")
        f.write(f"{onnx_results['median_ms']:.2f} ms | ")
        speedup_median = tf_results['median_ms'] / onnx_results['median_ms']
        f.write(f"{speedup_median:.2f}x |\n")

        f.write(f"| P95å»¶è¿Ÿ | {tf_results['p95_ms']:.2f} ms | ")
        f.write(f"{onnx_results['p95_ms']:.2f} ms | ")
        f.write(f"{speedup_p95:.2f}x |\n")

        f.write(f"| P99å»¶è¿Ÿ | {tf_results['p99_ms']:.2f} ms | ")
        f.write(f"{onnx_results['p99_ms']:.2f} ms | ")
        speedup_p99 = tf_results['p99_ms'] / onnx_results['p99_ms']
        f.write(f"{speedup_p99:.2f}x |\n")

        f.write("\n## ååé‡å¯¹æ¯”\n\n")
        f.write("| æ¡†æ¶ | ååé‡ (samples/s) |\n")
        f.write("|------|-------------------|\n")
        f.write(f"| TensorFlow | {tf_results['throughput_samples_per_sec']:.2f} |\n")
        f.write(f"| ONNX Runtime | {onnx_results['throughput_samples_per_sec']:.2f} |\n")

        throughput_speedup = onnx_results['throughput_samples_per_sec'] / tf_results['throughput_samples_per_sec']
        f.write(f"\n**ååé‡æå‡**: {throughput_speedup:.2f}x\n")

        f.write("\n## æ€»ç»“\n\n")
        if speedup_mean > 1:
            f.write(f"âœ… **ONNX Runtime å¹³å‡å»¶è¿Ÿæ›´ä½ï¼Œæé€Ÿ {speedup_mean:.2f}x**\n\n")
        else:
            f.write(f"âš ï¸ TensorFlow åœ¨æ­¤BERTæ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½\n\n")

        f.write("### å…³é”®å‘ç°\n\n")
        f.write(f"- å¹³å‡å»¶è¿Ÿæå‡: {speedup_mean:.2f}x\n")
        f.write(f"- P95å»¶è¿Ÿæå‡: {speedup_p95:.2f}x\n")
        f.write(f"- ååé‡æå‡: {throughput_speedup:.2f}x\n")

        f.write("\n### BERTæ¨¡å‹æ¨ç†æ€§èƒ½\n\n")
        f.write(f"- TensorFlowæ¯æ¬¡æ¨ç†: {tf_results['mean_ms']:.2f} ms\n")
        f.write(f"- ONNX Runtimeæ¯æ¬¡æ¨ç†: {onnx_results['mean_ms']:.2f} ms\n")
        f.write(f"- æ¨¡å‹å¤§å°: {conversion_info['model_size_mb']:.2f} MB\n")

        f.write("\n### å»ºè®®\n\n")
        if speedup_mean > 1.5:
            f.write("âœ… **å¼ºçƒˆæ¨èä½¿ç”¨ONNX Runtimeéƒ¨ç½²BERTæ¨¡å‹**\n\n")
            f.write("ONNX Runtimeåœ¨BERTæ¨¡å‹ä¸Šæœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œç‰¹åˆ«é€‚åˆï¼š\n")
            f.write("- ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡æ¨ç†\n")
            f.write("- å®æ—¶NLPåº”ç”¨\n")
            f.write("- è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²\n")
        elif speedup_mean > 1.1:
            f.write("âœ… **ONNX Runtimeæœ‰ä¸€å®šä¼˜åŠ¿**\n\n")
            f.write("å»ºè®®æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨ç†å¼•æ“\n")
        else:
            f.write("âš ï¸ **æ€§èƒ½å·®å¼‚ä¸æ˜æ˜¾**\n\n")
            f.write("å¯ä»¥æ ¹æ®éƒ¨ç½²ä¾¿åˆ©æ€§å’Œç”Ÿæ€ç³»ç»Ÿé€‰æ‹©æ¨ç†å¼•æ“\n")

    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return str(report_path)


def main():
    parser = argparse.ArgumentParser(description="BERTæ¨¡å‹ TensorFlow vs ONNXæ€§èƒ½å¯¹æ¯”")
    parser.add_argument("--model-size", default="lite", choices=["lite", "base"],
                       help="BERTæ¨¡å‹å¤§å°: lite(å¿«é€Ÿæµ‹è¯•) æˆ– base(å®Œæ•´BERT)")
    parser.add_argument("--seq-length", type=int, default=128,
                       help="åºåˆ—é•¿åº¦")
    parser.add_argument("--batch-size", type=int, default=1,
                       help="æ‰¹å¤§å°")
    parser.add_argument("--output-dir", default="results/bert_tf_vs_onnx",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num-runs", type=int, default=100,
                       help="æ€§èƒ½æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num-warmup", type=int, default=10,
                       help="çƒ­èº«è¿­ä»£æ¬¡æ•°")
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç¯å¢ƒæ£€æŸ¥
    env_info = check_environment()

    # åˆ›å»ºBERTæ¨¡å‹
    if args.model_size == "lite":
        model = create_bert_lite_model(seq_length=args.seq_length)
        model_config = {
            "æ¨¡å‹ç±»å‹": "BERT-Lite",
            "åºåˆ—é•¿åº¦": args.seq_length,
            "éšè—å±‚å¤§å°": 256,
            "Transformerå±‚æ•°": 4,
            "æ³¨æ„åŠ›å¤´æ•°": 4
        }
    else:
        model = create_bert_model(seq_length=args.seq_length)
        model_config = {
            "æ¨¡å‹ç±»å‹": "BERT-Base",
            "åºåˆ—é•¿åº¦": args.seq_length,
            "éšè—å±‚å¤§å°": 768,
            "Transformerå±‚æ•°": 12,
            "æ³¨æ„åŠ›å¤´æ•°": 12
        }

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print_section("åˆ›å»ºæµ‹è¯•æ•°æ®")
    test_data = np.random.randint(0, 10000, size=(args.batch_size, args.seq_length), dtype=np.int32)
    print(f"âœ“ æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")

    # æµ‹è¯•TensorFlowæ€§èƒ½
    tf_results = benchmark_tensorflow(
        model, test_data,
        num_runs=args.num_runs,
        num_warmup=args.num_warmup
    )

    # è½¬æ¢ä¸ºONNX
    onnx_path = output_dir / "bert_model.onnx"
    conversion_info = convert_to_onnx(model, onnx_path)

    if not conversion_info:
        print("âœ— ONNXè½¬æ¢å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
        return

    # æµ‹è¯•ONNXæ€§èƒ½
    onnx_results = benchmark_onnx(
        onnx_path, test_data,
        num_runs=args.num_runs,
        num_warmup=args.num_warmup
    )

    # ä¿å­˜ç»“æœ
    results = {
        "environment": env_info,
        "model_config": model_config,
        "tensorflow": tf_results,
        "onnx": onnx_results,
        "conversion": conversion_info
    }

    results_json = output_dir / "results.json"
    with open(results_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {results_json}")

    # ç”ŸæˆæŠ¥å‘Š
    report_path = generate_report(
        env_info, model_config, tf_results, onnx_results, conversion_info, output_dir
    )

    # æ‰“å°æ€»ç»“
    print_section("âœ“ BERTæµ‹è¯•å®Œæˆ!")
    print(f"\nç»“æœæ–‡ä»¶:")
    print(f"  - JSONç»“æœ: {results_json}")
    print(f"  - å¯¹æ¯”æŠ¥å‘Š: {report_path}")
    print(f"  - ONNXæ¨¡å‹: {onnx_path}")

    speedup = tf_results['mean_ms'] / onnx_results['mean_ms']
    print(f"\næ€§èƒ½æå‡: {speedup:.2f}x")

    if speedup > 1:
        print(f"âœ… ONNX Runtime æ¯” TensorFlow å¿« {speedup:.2f}x")
    else:
        print(f"âš ï¸ TensorFlow åœ¨æ­¤BERTæ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½")


if __name__ == "__main__":
    main()
