#!/usr/bin/env python3
"""
INT8 é‡åŒ–æ€§èƒ½ä¸å‡†ç¡®ç‡å¯¹æ¯”å·¥å…·

å¯¹æ¯”FP32åŸå§‹æ¨¡å‹å’ŒINT8é‡åŒ–æ¨¡å‹çš„ï¼š
1. æ¨ç†å»¶è¿Ÿå’Œååé‡
2. æ¨¡å‹å‡†ç¡®ç‡
3. æ¨¡å‹å¤§å°

Usage:
    python3 scripts/benchmark_int8_accuracy.py --model-type mobilenet
    python3 scripts/benchmark_int8_accuracy.py --model-type custom --model-path path/to/model
"""

import argparse
import json
import os
import time
from pathlib import Path

import numpy as np
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print("="*70)
print("INT8 é‡åŒ– vs FP32 å‡†ç¡®ç‡å’Œæ€§èƒ½å¯¹æ¯”")
print("="*70)
print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
print()


def create_test_model(model_type='mobilenet', input_shape=(224, 224, 3), num_classes=10):
    """
    åˆ›å»ºæµ‹è¯•æ¨¡å‹

    Args:
        model_type: æ¨¡å‹ç±»å‹ (mobilenet, simple_cnn)
        input_shape: è¾“å…¥å½¢çŠ¶
        num_classes: åˆ†ç±»æ•°

    Returns:
        ç¼–è¯‘åçš„æ¨¡å‹
    """
    print(f"åˆ›å»ºæµ‹è¯•æ¨¡å‹: {model_type}")

    if model_type == 'mobilenet':
        # ä½¿ç”¨MobileNetV2
        base_model = tf.keras.applications.MobileNetV2(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
        base_model.trainable = False

        model = tf.keras.Sequential([
            base_model,
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(num_classes, activation='softmax')
        ])

    elif model_type == 'simple_cnn':
        # ç®€å•CNNæ¨¡å‹
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=input_shape),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Conv2D(64, 3, activation='relu'),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Conv2D(64, 3, activation='relu'),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(num_classes, activation='softmax')
        ])

    else:
        raise ValueError(f"Unknown model type: {model_type}")

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print(f"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"  å‚æ•°æ€»æ•°: {model.count_params():,}")

    return model


def create_representative_dataset(input_shape, num_samples=100):
    """åˆ›å»ºä»£è¡¨æ€§æ•°æ®é›†ç”¨äºé‡åŒ–æ ¡å‡†"""
    def representative_dataset_gen():
        for _ in range(num_samples):
            data = np.random.random_sample((1,) + input_shape).astype(np.float32)
            yield [data]
    return representative_dataset_gen


def quantize_to_int8(model, input_shape, output_path, num_calibration_samples=100):
    """
    å°†æ¨¡å‹é‡åŒ–ä¸ºINT8 TFLite

    Args:
        model: Kerasæ¨¡å‹
        input_shape: è¾“å…¥å½¢çŠ¶
        output_path: è¾“å‡ºè·¯å¾„
        num_calibration_samples: æ ¡å‡†æ ·æœ¬æ•°

    Returns:
        é‡åŒ–æ¨¡å‹è·¯å¾„
    """
    print(f"\né‡åŒ–æ¨¡å‹ä¸ºINT8...")

    # åˆ›å»ºä»£è¡¨æ€§æ•°æ®é›†
    representative_dataset_gen = create_representative_dataset(
        input_shape,
        num_calibration_samples
    )

    # åˆ›å»ºè½¬æ¢å™¨
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset_gen

    # INT8é‡åŒ–è®¾ç½®
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

    try:
        tflite_model = converter.convert()

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'wb') as f:
            f.write(tflite_model)

        size_mb = len(tflite_model) / (1024 * 1024)
        print(f"âœ“ INT8é‡åŒ–å®Œæˆ")
        print(f"  å¤§å°: {size_mb:.2f} MB")
        print(f"  è·¯å¾„: {output_path}")

        return output_path

    except Exception as e:
        print(f"âœ— INT8é‡åŒ–å¤±è´¥: {e}")
        print("\nå°è¯•åŠ¨æ€èŒƒå›´é‡åŒ–ä½œä¸ºæ›¿ä»£...")
        return quantize_dynamic(model, output_path)


def quantize_dynamic(model, output_path):
    """åŠ¨æ€èŒƒå›´é‡åŒ–ï¼ˆæƒé‡INT8ï¼Œæ¿€æ´»FP32ï¼‰"""
    print(f"\nåŠ¨æ€èŒƒå›´é‡åŒ–...")

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]

    tflite_model = converter.convert()

    output_path = Path(output_path)
    with open(output_path, 'wb') as f:
        f.write(tflite_model)

    size_mb = len(tflite_model) / (1024 * 1024)
    print(f"âœ“ åŠ¨æ€é‡åŒ–å®Œæˆ")
    print(f"  å¤§å°: {size_mb:.2f} MB")

    return output_path


def benchmark_keras_model(model, test_data, test_labels, num_warmup=10, num_test=50):
    """
    æµ‹è¯•Kerasæ¨¡å‹æ€§èƒ½

    Returns:
        dict: åŒ…å«å»¶è¿Ÿã€ååé‡ã€å‡†ç¡®ç‡çš„ç»“æœ
    """
    print(f"\n{'='*70}")
    print("æµ‹è¯• FP32 Keras æ¨¡å‹")
    print(f"{'='*70}")

    # Warmup
    print(f"\nçƒ­èº«: {num_warmup} iterations...")
    for i in range(num_warmup):
        _ = model(test_data[i:i+1], training=False)
        if (i + 1) % 5 == 0:
            print(f"  Warmup: {i+1}/{num_warmup}")

    # å»¶è¿Ÿæµ‹è¯•
    print(f"\nå»¶è¿Ÿæµ‹è¯•: {num_test} iterations...")
    latencies = []

    for i in range(num_test):
        start = time.perf_counter()
        _ = model(test_data[i:i+1], training=False)
        end = time.perf_counter()

        latency_ms = (end - start) * 1000
        latencies.append(latency_ms)

        if (i + 1) % 10 == 0:
            print(f"  æµ‹è¯•: {i+1}/{num_test}")

    latencies = np.array(latencies)

    # å‡†ç¡®ç‡æµ‹è¯•
    print(f"\nå‡†ç¡®ç‡æµ‹è¯•...")
    predictions = model.predict(test_data, verbose=0)
    predicted_classes = np.argmax(predictions, axis=1)
    accuracy = np.mean(predicted_classes == test_labels)

    results = {
        "model_type": "FP32 Keras",
        "latency_mean_ms": float(np.mean(latencies)),
        "latency_median_ms": float(np.median(latencies)),
        "latency_std_ms": float(np.std(latencies)),
        "latency_p95_ms": float(np.percentile(latencies, 95)),
        "latency_p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": num_test / (np.sum(latencies) / 1000),
        "accuracy": float(accuracy),
    }

    print(f"\nâœ“ FP32 Keras æµ‹è¯•å®Œæˆ")
    print(f"  å»¶è¿Ÿ (mean): {results['latency_mean_ms']:.2f} ms")
    print(f"  å»¶è¿Ÿ (p95):  {results['latency_p95_ms']:.2f} ms")
    print(f"  ååé‡:      {results['throughput_samples_per_sec']:.2f} samples/sec")
    print(f"  å‡†ç¡®ç‡:      {results['accuracy']*100:.2f}%")

    return results


def benchmark_tflite_model(tflite_path, test_data, test_labels, num_warmup=10, num_test=50):
    """
    æµ‹è¯•TFLiteæ¨¡å‹æ€§èƒ½

    Returns:
        dict: åŒ…å«å»¶è¿Ÿã€ååé‡ã€å‡†ç¡®ç‡çš„ç»“æœ
    """
    print(f"\n{'='*70}")
    print("æµ‹è¯• INT8 TFLite æ¨¡å‹")
    print(f"{'='*70}")

    # åŠ è½½TFLiteæ¨¡å‹
    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  è¾“å…¥: {input_details[0]['shape']}, {input_details[0]['dtype']}")
    print(f"  è¾“å‡º: {output_details[0]['shape']}, {output_details[0]['dtype']}")

    # è·å–è¾“å…¥è¾“å‡ºçš„ç¼©æ”¾å‚æ•°ï¼ˆç”¨äºINT8ï¼‰
    input_scale, input_zero_point = input_details[0]['quantization']
    output_scale, output_zero_point = output_details[0]['quantization']

    # Warmup
    print(f"\nçƒ­èº«: {num_warmup} iterations...")
    for i in range(num_warmup):
        # é‡åŒ–è¾“å…¥
        input_data = test_data[i:i+1]
        if input_details[0]['dtype'] == np.uint8:
            input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)
        elif input_details[0]['dtype'] == np.int8:
            input_data = (input_data / input_scale + input_zero_point).astype(np.int8)

        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()

        if (i + 1) % 5 == 0:
            print(f"  Warmup: {i+1}/{num_warmup}")

    # å»¶è¿Ÿæµ‹è¯•
    print(f"\nå»¶è¿Ÿæµ‹è¯•: {num_test} iterations...")
    latencies = []
    predictions = []

    for i in range(num_test):
        # å‡†å¤‡è¾“å…¥
        input_data = test_data[i:i+1]
        if input_details[0]['dtype'] == np.uint8:
            input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)
        elif input_details[0]['dtype'] == np.int8:
            input_data = (input_data / input_scale + input_zero_point).astype(np.int8)

        start = time.perf_counter()
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
        end = time.perf_counter()

        latency_ms = (end - start) * 1000
        latencies.append(latency_ms)

        # åé‡åŒ–è¾“å‡º
        if output_details[0]['dtype'] == np.uint8 or output_details[0]['dtype'] == np.int8:
            output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale

        predictions.append(np.argmax(output_data))

        if (i + 1) % 10 == 0:
            print(f"  æµ‹è¯•: {i+1}/{num_test}")

    latencies = np.array(latencies)
    predictions = np.array(predictions)

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(predictions == test_labels[:num_test])

    results = {
        "model_type": "INT8 TFLite",
        "latency_mean_ms": float(np.mean(latencies)),
        "latency_median_ms": float(np.median(latencies)),
        "latency_std_ms": float(np.std(latencies)),
        "latency_p95_ms": float(np.percentile(latencies, 95)),
        "latency_p99_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": num_test / (np.sum(latencies) / 1000),
        "accuracy": float(accuracy),
    }

    print(f"\nâœ“ INT8 TFLite æµ‹è¯•å®Œæˆ")
    print(f"  å»¶è¿Ÿ (mean): {results['latency_mean_ms']:.2f} ms")
    print(f"  å»¶è¿Ÿ (p95):  {results['latency_p95_ms']:.2f} ms")
    print(f"  ååé‡:      {results['throughput_samples_per_sec']:.2f} samples/sec")
    print(f"  å‡†ç¡®ç‡:      {results['accuracy']*100:.2f}%")

    return results


def generate_comparison_report(fp32_results, int8_results, output_dir):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    print(f"\n{'='*70}")
    print("ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
    print(f"{'='*70}")

    report_file = output_dir / "int8_vs_fp32_report.md"

    with open(report_file, "w") as f:
        f.write("# INT8 é‡åŒ– vs FP32 æ€§èƒ½å’Œå‡†ç¡®ç‡å¯¹æ¯”\n\n")

        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | FP32 | INT8 | å˜åŒ– |\n")
        f.write("|------|------|------|------|\n")

        # å»¶è¿Ÿå¯¹æ¯”
        speedup_mean = fp32_results['latency_mean_ms'] / int8_results['latency_mean_ms']
        speedup_p95 = fp32_results['latency_p95_ms'] / int8_results['latency_p95_ms']

        f.write(f"| å»¶è¿Ÿ (mean) | {fp32_results['latency_mean_ms']:.2f} ms | "
                f"{int8_results['latency_mean_ms']:.2f} ms | "
                f"{speedup_mean:.2f}x ğŸš€ |\n")

        f.write(f"| å»¶è¿Ÿ (median) | {fp32_results['latency_median_ms']:.2f} ms | "
                f"{int8_results['latency_median_ms']:.2f} ms | "
                f"{fp32_results['latency_median_ms']/int8_results['latency_median_ms']:.2f}x |\n")

        f.write(f"| å»¶è¿Ÿ (p95) | {fp32_results['latency_p95_ms']:.2f} ms | "
                f"{int8_results['latency_p95_ms']:.2f} ms | "
                f"{speedup_p95:.2f}x |\n")

        # ååé‡å¯¹æ¯”
        throughput_improvement = int8_results['throughput_samples_per_sec'] / fp32_results['throughput_samples_per_sec']

        f.write(f"| ååé‡ | {fp32_results['throughput_samples_per_sec']:.2f} samples/s | "
                f"{int8_results['throughput_samples_per_sec']:.2f} samples/s | "
                f"{throughput_improvement:.2f}x ğŸ“ˆ |\n")

        f.write("\n## å‡†ç¡®ç‡å¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | FP32 | INT8 | å·®å¼‚ |\n")
        f.write("|------|------|------|------|\n")

        accuracy_diff = (fp32_results['accuracy'] - int8_results['accuracy']) * 100

        f.write(f"| å‡†ç¡®ç‡ | {fp32_results['accuracy']*100:.2f}% | "
                f"{int8_results['accuracy']*100:.2f}% | "
                f"{accuracy_diff:+.2f}% |\n")

        f.write("\n## æ€»ç»“\n\n")

        if speedup_mean > 1.0:
            f.write(f"âœ… **INT8é‡åŒ–æé€Ÿ {speedup_mean:.2f}x**\n\n")
        else:
            f.write(f"âš ï¸ **INT8é‡åŒ–æœªæé€Ÿ** ({1/speedup_mean:.2f}x slower)\n\n")

        if abs(accuracy_diff) < 1.0:
            f.write(f"âœ… **å‡†ç¡®ç‡æŸå¤±å¯å¿½ç•¥** ({accuracy_diff:+.2f}%)\n\n")
        elif accuracy_diff > 0 and accuracy_diff < 3.0:
            f.write(f"âš ï¸ **å‡†ç¡®ç‡è½»å¾®ä¸‹é™** ({accuracy_diff:+.2f}%)\n\n")
        else:
            f.write(f"âŒ **å‡†ç¡®ç‡æ˜æ˜¾ä¸‹é™** ({accuracy_diff:+.2f}%)\n\n")

        f.write("### å…³é”®æŒ‡æ ‡\n\n")
        f.write(f"- å¹³å‡å»¶è¿Ÿæå‡: **{speedup_mean:.2f}x**\n")
        f.write(f"- P95å»¶è¿Ÿæå‡: **{speedup_p95:.2f}x**\n")
        f.write(f"- ååé‡æå‡: **{throughput_improvement:.2f}x**\n")
        f.write(f"- å‡†ç¡®ç‡å˜åŒ–: **{accuracy_diff:+.2f}%**\n\n")

        f.write("## å»ºè®®\n\n")

        if speedup_mean > 1.5 and abs(accuracy_diff) < 2.0:
            f.write("âœ… **æ¨èä½¿ç”¨INT8é‡åŒ–** - æ€§èƒ½æå‡æ˜¾è‘—ä¸”å‡†ç¡®ç‡æŸå¤±å¯æ¥å—\n")
        elif speedup_mean > 1.2 and abs(accuracy_diff) < 1.0:
            f.write("âœ… **å¯ä»¥ä½¿ç”¨INT8é‡åŒ–** - æ€§èƒ½å’Œå‡†ç¡®ç‡éƒ½åœ¨å¯æ¥å—èŒƒå›´\n")
        elif abs(accuracy_diff) > 3.0:
            f.write("âš ï¸ **è°¨æ…ä½¿ç”¨INT8é‡åŒ–** - å‡†ç¡®ç‡ä¸‹é™è¾ƒå¤šï¼Œå»ºè®®é‡æ–°æ ¡å‡†æˆ–ä½¿ç”¨Float16é‡åŒ–\n")
        else:
            f.write("âš ï¸ **è¯„ä¼°ä½¿ç”¨åœºæ™¯** - æ ¹æ®åº”ç”¨éœ€æ±‚æƒè¡¡æ€§èƒ½å’Œå‡†ç¡®ç‡\n")

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return report_file


def main():
    parser = argparse.ArgumentParser(description="INT8 vs FP32 Benchmark")
    parser.add_argument("--model-type", type=str, default="simple_cnn",
                        choices=['mobilenet', 'simple_cnn', 'custom'],
                        help="Model type to test")
    parser.add_argument("--model-path", type=str, help="Path to custom model")
    parser.add_argument("--input-shape", type=str, default="28,28,1",
                        help="Input shape (comma-separated)")
    parser.add_argument("--num-classes", type=int, default=10, help="Number of classes")
    parser.add_argument("--num-test-samples", type=int, default=100, help="Number of test samples")
    parser.add_argument("--num-warmup", type=int, default=10, help="Warmup iterations")
    parser.add_argument("--num-test", type=int, default=50, help="Test iterations")
    parser.add_argument("--output", type=str, default="./results/int8_benchmark",
                        help="Output directory")

    args = parser.parse_args()

    # è§£æè¾“å…¥å½¢çŠ¶
    input_shape = tuple(map(int, args.input_shape.split(',')))

    print(f"é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"  è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"  ç±»åˆ«æ•°: {args.num_classes}")
    print(f"  æµ‹è¯•æ ·æœ¬: {args.num_test_samples}")

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    if args.model_type == 'custom' and args.model_path:
        print(f"\nåŠ è½½è‡ªå®šä¹‰æ¨¡å‹: {args.model_path}")
        model = tf.keras.models.load_model(args.model_path)
    else:
        model = create_test_model(args.model_type, input_shape, args.num_classes)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print(f"\nåˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_data = np.random.random((args.num_test_samples,) + input_shape).astype(np.float32)
    test_labels = np.random.randint(0, args.num_classes, args.num_test_samples)
    print(f"âœ“ æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ: {test_data.shape}")

    # ä¿å­˜FP32æ¨¡å‹
    fp32_model_path = output_dir / "model_fp32.h5"
    model.save(fp32_model_path)
    fp32_size_mb = os.path.getsize(fp32_model_path) / (1024 * 1024)
    print(f"\nâœ“ FP32æ¨¡å‹å·²ä¿å­˜: {fp32_model_path}")
    print(f"  å¤§å°: {fp32_size_mb:.2f} MB")

    # é‡åŒ–ä¸ºINT8
    int8_model_path = output_dir / "model_int8.tflite"
    quantize_to_int8(model, input_shape, int8_model_path)

    int8_size_mb = os.path.getsize(int8_model_path) / (1024 * 1024)
    compression_ratio = fp32_size_mb / int8_size_mb
    print(f"\nâœ“ æ¨¡å‹å¤§å°å¯¹æ¯”:")
    print(f"  FP32: {fp32_size_mb:.2f} MB")
    print(f"  INT8: {int8_size_mb:.2f} MB")
    print(f"  å‹ç¼©æ¯”: {compression_ratio:.2f}x")

    # æµ‹è¯•FP32æ¨¡å‹
    fp32_results = benchmark_keras_model(
        model, test_data, test_labels,
        args.num_warmup, args.num_test
    )

    # æµ‹è¯•INT8æ¨¡å‹
    int8_results = benchmark_tflite_model(
        int8_model_path, test_data, test_labels,
        args.num_warmup, args.num_test
    )

    # ä¿å­˜ç»“æœ
    results = {
        "fp32": fp32_results,
        "int8": int8_results,
        "model_size_mb": {
            "fp32": fp32_size_mb,
            "int8": int8_size_mb,
            "compression_ratio": compression_ratio
        }
    }

    results_file = output_dir / "results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {results_file}")

    # ç”ŸæˆæŠ¥å‘Š
    report_file = generate_comparison_report(fp32_results, int8_results, output_dir)

    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*70}")
    print("âœ“ INT8 vs FP32 å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print(f"{'='*70}")
    print(f"\nç»“æœæ–‡ä»¶:")
    print(f"  - FP32æ¨¡å‹: {fp32_model_path}")
    print(f"  - INT8æ¨¡å‹: {int8_model_path}")
    print(f"  - æµ‹è¯•ç»“æœ: {results_file}")
    print(f"  - å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    print(f"\n{'='*70}")


if __name__ == "__main__":
    main()
