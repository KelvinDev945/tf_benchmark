#!/usr/bin/env python3
"""
å¯¹æ¯”tf2onnxå’ŒOptimumè½¬æ¢BERTæ¨¡å‹åçš„ONNXæ¨ç†æ€§èƒ½

ä½¿ç”¨ç›¸åŒçš„BERTæ¨¡å‹ï¼Œåˆ†åˆ«ç”¨ä¸¤ç§å·¥å…·è½¬æ¢ï¼Œç„¶åå¯¹æ¯”æ¨ç†é€Ÿåº¦
"""

import os
import shutil
import tempfile
import time
from pathlib import Path

import numpy as np


def print_section(title):
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80)


def method1_optimum_bert():
    """æ–¹æ³•1: ä½¿ç”¨Optimumè½¬æ¢BERT"""
    print_section("æ–¹æ³•1: Optimumè½¬æ¢BERT")

    import onnxruntime as ort  # noqa: F401
    from optimum.onnxruntime import ORTModelForSequenceClassification
    from transformers import AutoTokenizer

    model_name = "prajjwal1/bert-tiny"  # ä½¿ç”¨å°æ¨¡å‹å¿«é€Ÿæµ‹è¯•

    print(f"1. ä½¿ç”¨OptimumåŠ è½½å’Œè½¬æ¢: {model_name}")
    start_time = time.time()

    # Optimumè‡ªåŠ¨è½¬æ¢ä¸ºONNX
    model = ORTModelForSequenceClassification.from_pretrained(model_name, export=True)

    conversion_time = time.time() - start_time

    # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    model.save_pretrained(temp_dir)

    onnx_path = Path(temp_dir) / "model.onnx"
    file_size_mb = onnx_path.stat().st_size / (1024 * 1024)

    print(f"âœ“ è½¬æ¢å®Œæˆ!")
    print(f"  è½¬æ¢æ—¶é—´: {conversion_time:.2f} ç§’")
    print(f"  æ¨¡å‹å¤§å°: {file_size_mb:.2f} MB")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    test_text = "This is a test sentence for benchmarking."
    inputs = tokenizer(test_text, return_tensors="pt", padding=True, max_length=128)

    # æ¨ç†æ€§èƒ½æµ‹è¯•
    print("\n2. æ¨ç†æ€§èƒ½æµ‹è¯• (Optimum)")
    num_runs = 1000
    num_warmup = 100

    # é¢„çƒ­
    print(f"  é¢„çƒ­ä¸­ ({num_warmup} æ¬¡)...")
    for i in range(num_warmup):
        if (i + 1) % 20 == 0:
            print(f"    é¢„çƒ­è¿›åº¦: {i+1}/{num_warmup}")
        _ = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])

    # åŸºå‡†æµ‹è¯•
    print(f"  å¼€å§‹æ¨ç†æµ‹è¯• ({num_runs} æ¬¡)...")
    latencies = []
    for i in range(num_runs):
        if (i + 1) % 200 == 0:
            print(f"    æ¨ç†è¿›åº¦: {i+1}/{num_runs}")
        start = time.perf_counter()
        _ = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
        latencies.append((time.perf_counter() - start) * 1000)

    mean_latency = np.mean(latencies)
    p95_latency = np.percentile(latencies, 95)

    print(f"  å¹³å‡å»¶è¿Ÿ: {mean_latency:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f} ms")
    print(f"  ååé‡: {1000/mean_latency:.2f} samples/sec")

    # æ¸…ç†
    shutil.rmtree(temp_dir, ignore_errors=True)

    return {
        "method": "Optimum",
        "conversion_time": conversion_time,
        "model_size_mb": file_size_mb,
        "mean_latency_ms": mean_latency,
        "p95_latency_ms": p95_latency,
        "throughput": 1000 / mean_latency,
        "onnx_path": str(onnx_path),
    }


def method2_tf2onnx_bert():
    """æ–¹æ³•2: ä½¿ç”¨tf2onnxè½¬æ¢BERT"""
    print_section("æ–¹æ³•2: tf2onnxè½¬æ¢BERT")

    try:
        import subprocess

        import onnxruntime as ort
        import tensorflow as tf
        import tf2onnx  # noqa: F401
        from transformers import BertTokenizer, TFBertForSequenceClassification

        model_name = "prajjwal1/bert-tiny"

        print(f"1. åŠ è½½TensorFlow BERTæ¨¡å‹: {model_name}")
        model = TFBertForSequenceClassification.from_pretrained(model_name, from_pt=True)

        # ä¿å­˜ä¸ºSavedModel
        temp_dir = tempfile.mkdtemp()
        saved_model_path = os.path.join(temp_dir, "saved_model")

        print("2. å¯¼å‡ºä¸ºSavedModel...")
        # ä½¿ç”¨exportè€Œä¸æ˜¯save (Keras 3å…¼å®¹)
        tf.saved_model.save(model, saved_model_path)

        # ä½¿ç”¨tf2onnxè½¬æ¢
        onnx_path = os.path.join(temp_dir, "model.onnx")

        print("3. ä½¿ç”¨tf2onnxè½¬æ¢...")
        start_time = time.time()

        cmd = [
            "python3",
            "-m",
            "tf2onnx.convert",
            "--saved-model",
            saved_model_path,
            "--output",
            onnx_path,
            "--opset",
            "15",
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"âœ— tf2onnxè½¬æ¢å¤±è´¥:")
            print(result.stderr[:500])
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None

        conversion_time = time.time() - start_time
        file_size_mb = Path(onnx_path).stat().st_size / (1024 * 1024)

        print(f"âœ“ è½¬æ¢å®Œæˆ!")
        print(f"  è½¬æ¢æ—¶é—´: {conversion_time:.2f} ç§’")
        print(f"  æ¨¡å‹å¤§å°: {file_size_mb:.2f} MB")

        # æ¨ç†æ€§èƒ½æµ‹è¯•
        print("\n4. æ¨ç†æ€§èƒ½æµ‹è¯• (tf2onnx)")

        tokenizer = BertTokenizer.from_pretrained(model_name)
        test_text = "This is a test sentence for benchmarking."
        inputs = tokenizer(test_text, return_tensors="np", padding=True, max_length=128)

        # åˆ›å»ºONNX Runtimeä¼šè¯
        sess = ort.InferenceSession(onnx_path)

        # è·å–è¾“å…¥åç§°
        input_names = [inp.name for inp in sess.get_inputs()]
        print(f"  ONNXæ¨¡å‹éœ€è¦çš„è¾“å…¥: {input_names}")

        # å‡†å¤‡è¾“å…¥å­—å…¸ - æ™ºèƒ½åŒ¹é…è¾“å…¥åç§°
        onnx_inputs = {}

        # ä¸ºæ¯ä¸ªONNXè¾“å…¥æ‰¾åˆ°å¯¹åº”çš„tokenizerè¾“å‡º
        # TensorFlowæ¨¡å‹é€šå¸¸ä½¿ç”¨int32ï¼Œè€ŒPyTorchä½¿ç”¨int64
        for onnx_input_name in input_names:
            lower_name = onnx_input_name.lower()

            # åŒ¹é…input_ids
            if "input" in lower_name and "ids" in lower_name:
                onnx_inputs[onnx_input_name] = inputs["input_ids"].astype(np.int32)
            # åŒ¹é…attention_mask
            elif "attention" in lower_name or "mask" in lower_name:
                onnx_inputs[onnx_input_name] = inputs["attention_mask"].astype(np.int32)
            # åŒ¹é…token_type_ids
            elif "token_type" in lower_name or "segment" in lower_name or "type" in lower_name:
                if "token_type_ids" in inputs:
                    onnx_inputs[onnx_input_name] = inputs["token_type_ids"].astype(np.int32)
                else:
                    # å¦‚æœtokenizeræ²¡æœ‰æä¾›ï¼Œåˆ›å»ºå…¨0æ•°ç»„
                    onnx_inputs[onnx_input_name] = np.zeros_like(
                        inputs["input_ids"], dtype=np.int32
                    )

        print(f"  æä¾›çš„è¾“å…¥: {list(onnx_inputs.keys())}")

        num_runs = 1000
        num_warmup = 100

        # é¢„çƒ­
        print(f"\n  é¢„çƒ­ä¸­ ({num_warmup} æ¬¡)...")
        for i in range(num_warmup):
            if (i + 1) % 20 == 0:
                print(f"    é¢„çƒ­è¿›åº¦: {i+1}/{num_warmup}")
            _ = sess.run(None, onnx_inputs)

        # åŸºå‡†æµ‹è¯•
        print(f"  å¼€å§‹æ¨ç†æµ‹è¯• ({num_runs} æ¬¡)...")
        latencies = []
        for i in range(num_runs):
            if (i + 1) % 200 == 0:
                print(f"    æ¨ç†è¿›åº¦: {i+1}/{num_runs}")
            start = time.perf_counter()
            _ = sess.run(None, onnx_inputs)
            latencies.append((time.perf_counter() - start) * 1000)

        mean_latency = np.mean(latencies)
        p95_latency = np.percentile(latencies, 95)

        print(f"  å¹³å‡å»¶è¿Ÿ: {mean_latency:.2f} ms")
        print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f} ms")
        print(f"  ååé‡: {1000/mean_latency:.2f} samples/sec")

        # æ¸…ç†
        shutil.rmtree(temp_dir, ignore_errors=True)

        return {
            "method": "tf2onnx",
            "conversion_time": conversion_time,
            "model_size_mb": file_size_mb,
            "mean_latency_ms": mean_latency,
            "p95_latency_ms": p95_latency,
            "throughput": 1000 / mean_latency,
            "onnx_path": onnx_path,
        }

    except Exception as e:
        print(f"âœ— tf2onnxæ–¹æ³•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return None


def main():
    print("=" * 80)
    print("BERTæ¨¡å‹: tf2onnx vs Optimum ONNXè½¬æ¢æ¨ç†æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    print("æ¨¡å‹: prajjwal1/bert-tiny (2å±‚BERT)")
    print("ä»»åŠ¡: åºåˆ—åˆ†ç±»")

    # æµ‹è¯•æ–¹æ³•1: Optimum
    try:
        optimum_result = method1_optimum_bert()
    except Exception as e:
        print(f"\nâœ— Optimumæµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        optimum_result = None

    # æµ‹è¯•æ–¹æ³•2: tf2onnx
    try:
        tf2onnx_result = method2_tf2onnx_bert()
    except Exception as e:
        print(f"\nâœ— tf2onnxæµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        tf2onnx_result = None

    # å¯¹æ¯”æ€»ç»“
    print_section("æ€§èƒ½å¯¹æ¯”æ€»ç»“")

    if optimum_result and tf2onnx_result:
        print("\nâœ… ä¸¤ç§æ–¹æ³•éƒ½æˆåŠŸ!\n")

        print("è½¬æ¢æ€§èƒ½å¯¹æ¯”:")
        print(f"  Optimumè½¬æ¢æ—¶é—´:  {optimum_result['conversion_time']:6.2f} ç§’")
        print(f"  tf2onnxè½¬æ¢æ—¶é—´:  {tf2onnx_result['conversion_time']:6.2f} ç§’")
        print(
            f"  å·®å¼‚: {abs(optimum_result['conversion_time'] - tf2onnx_result['conversion_time']):.2f} ç§’"  # noqa: E501
        )

        print(f"\næ¨¡å‹å¤§å°å¯¹æ¯”:")
        print(f"  Optimumæ¨¡å‹å¤§å°:  {optimum_result['model_size_mb']:6.2f} MB")
        print(f"  tf2onnxæ¨¡å‹å¤§å°:  {tf2onnx_result['model_size_mb']:6.2f} MB")

        print(f"\næ¨ç†æ€§èƒ½å¯¹æ¯” (é‡è¦!):")
        print(f"  {'æŒ‡æ ‡':<20} {'Optimum':>12} {'tf2onnx':>12} {'å·®å¼‚':>12}")
        print(f"  {'-'*20} {'-'*12} {'-'*12} {'-'*12}")
        print(
            f"  {'å¹³å‡å»¶è¿Ÿ (ms)':<20} {optimum_result['mean_latency_ms']:>12.2f} {tf2onnx_result['mean_latency_ms']:>12.2f} {abs(optimum_result['mean_latency_ms'] - tf2onnx_result['mean_latency_ms']):>12.2f}"  # noqa: E501
        )
        print(
            f"  {'P95å»¶è¿Ÿ (ms)':<20} {optimum_result['p95_latency_ms']:>12.2f} {tf2onnx_result['p95_latency_ms']:>12.2f} {abs(optimum_result['p95_latency_ms'] - tf2onnx_result['p95_latency_ms']):>12.2f}"  # noqa: E501
        )
        print(
            f"  {'ååé‡ (samples/s)':<20} {optimum_result['throughput']:>12.2f} {tf2onnx_result['throughput']:>12.2f} {abs(optimum_result['throughput'] - tf2onnx_result['throughput']):>12.2f}"  # noqa: E501
        )

        # è®¡ç®—æ€§èƒ½å·®å¼‚ç™¾åˆ†æ¯”
        latency_diff_pct = (
            abs(optimum_result["mean_latency_ms"] - tf2onnx_result["mean_latency_ms"])
            / min(optimum_result["mean_latency_ms"], tf2onnx_result["mean_latency_ms"])
            * 100
        )

        print(f"\nç»“è®º:")
        if latency_diff_pct < 5:
            print(f"  ğŸ“Š æ¨ç†æ€§èƒ½åŸºæœ¬ç›¸åŒ (å·®å¼‚ < 5%: {latency_diff_pct:.1f}%)")
            print("  â†’ ä¸¤ç§è½¬æ¢æ–¹æ³•äº§ç”Ÿçš„ONNXæ¨¡å‹æ¨ç†é€Ÿåº¦æ²¡æœ‰æ˜¾è‘—å·®å¼‚")
            print("  â†’ é€‰æ‹©è½¬æ¢æ–¹æ³•åº”åŸºäºå·¥å…·å…¼å®¹æ€§å’Œæ˜“ç”¨æ€§ï¼Œè€Œéæ¨ç†æ€§èƒ½")
        elif latency_diff_pct < 10:
            print(f"  ğŸ“Š æ¨ç†æ€§èƒ½ç•¥æœ‰å·®å¼‚ (5-10%: {latency_diff_pct:.1f}%)")
            if optimum_result["mean_latency_ms"] < tf2onnx_result["mean_latency_ms"]:
                print("  â†’ Optimumè½¬æ¢çš„æ¨¡å‹ç•¥å¿«")
            else:
                print("  â†’ tf2onnxè½¬æ¢çš„æ¨¡å‹ç•¥å¿«")
        else:
            print(f"  ğŸ“Š æ¨ç†æ€§èƒ½æœ‰æ˜æ˜¾å·®å¼‚ (> 10%: {latency_diff_pct:.1f}%)")
            if optimum_result["mean_latency_ms"] < tf2onnx_result["mean_latency_ms"]:
                faster_pct = (
                    tf2onnx_result["mean_latency_ms"] / optimum_result["mean_latency_ms"] - 1
                ) * 100
                print(f"  â†’ Optimumè½¬æ¢çš„æ¨¡å‹å¿« {faster_pct:.1f}%")
            else:
                faster_pct = (
                    optimum_result["mean_latency_ms"] / tf2onnx_result["mean_latency_ms"] - 1
                ) * 100
                print(f"  â†’ tf2onnxè½¬æ¢çš„æ¨¡å‹å¿« {faster_pct:.1f}%")

        print("\næ¨è:")
        print("  â€¢ å¯¹äºHuggingFaceæ¨¡å‹ â†’ ä½¿ç”¨Optimum (æ›´ç®€å•ï¼Œæ— ç‰ˆæœ¬å†²çª)")
        print("  â€¢ å¯¹äºTensorFlowè‡ªå®šä¹‰æ¨¡å‹ â†’ ä½¿ç”¨tf2onnx (å…¼å®¹æ€§é—®é¢˜å¾…è§£å†³)")

    elif optimum_result:
        print("\nâœ“ ä»…OptimumæˆåŠŸ")
        print(f"  æ¨ç†å»¶è¿Ÿ: {optimum_result['mean_latency_ms']:.2f} ms")
        print("  â†’ tf2onnxåœ¨å½“å‰ç¯å¢ƒé‡åˆ°å…¼å®¹æ€§é—®é¢˜")

    elif tf2onnx_result:
        print("\nâœ“ ä»…tf2onnxæˆåŠŸ")
        print(f"  æ¨ç†å»¶è¿Ÿ: {tf2onnx_result['mean_latency_ms']:.2f} ms")

    else:
        print("\nâŒ ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥")
        print("  å¯èƒ½åŸå› : ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")


if __name__ == "__main__":
    main()
