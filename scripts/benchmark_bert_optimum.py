#!/usr/bin/env python3
"""
BERTæ¨¡å‹ TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯”æµ‹è¯• (ä½¿ç”¨HuggingFace Optimum)

ä½¿ç”¨Optimumè¿›è¡ŒONNXè½¬æ¢ï¼Œé¿å…tf2onnxçš„protobufç‰ˆæœ¬å†²çª
è§£å†³ TODO.md Issue #3 - ONNXè½¬æ¢å¤±è´¥é—®é¢˜
"""

import argparse
import json
import sys
import time
from pathlib import Path

import numpy as np


def print_section(title):
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print_section("ç¯å¢ƒæ£€æŸ¥")

    env_info = {}

    import tensorflow as tf

    env_info["tensorflow"] = tf.__version__
    print(f"âœ“ TensorFlow: {tf.__version__}")

    import numpy as np

    env_info["numpy"] = np.__version__
    print(f"âœ“ NumPy: {np.__version__}")

    try:
        import onnxruntime as ort

        env_info["onnxruntime"] = ort.__version__
        print(f"âœ“ ONNX Runtime: {ort.__version__}")
    except Exception as e:
        print(f"âœ— ONNX Runtime: {e}")
        sys.exit(1)

    try:
        import optimum

        env_info["optimum"] = optimum.__version__
        print(f"âœ“ Optimum: {optimum.__version__}")
    except Exception as e:
        print(f"âœ— Optimum: {e}")
        print("è¯·å®‰è£…: pip install optimum[onnxruntime]")
        sys.exit(1)

    try:
        import transformers

        env_info["transformers"] = transformers.__version__
        print(f"âœ“ Transformers: {transformers.__version__}")
    except Exception as e:
        print(f"âœ— Transformers: {e}")
        sys.exit(1)

    try:
        import google.protobuf

        env_info["protobuf"] = google.protobuf.__version__
        print(f"âœ“ Protobuf: {google.protobuf.__version__}")
    except Exception as e:
        print(f"âœ— Protobuf: {e}")

    return env_info


def benchmark_tensorflow_bert(model_name="bert-base-uncased", num_runs=100, num_warmup=10):
    """
    TensorFlow BERTåŸºå‡†æµ‹è¯•

    å‚æ•°:
        model_name: HuggingFaceæ¨¡å‹åç§°
        num_runs: æµ‹è¯•è¿è¡Œæ¬¡æ•°
        num_warmup: é¢„çƒ­è¿è¡Œæ¬¡æ•°
    """
    print_section(f"TensorFlow BERT åŸºå‡†æµ‹è¯• ({model_name})")

    from transformers import BertTokenizer, TFBertForSequenceClassification

    # åŠ è½½æ¨¡å‹å’Œtokenizer
    print("åŠ è½½TensorFlow BERTæ¨¡å‹...")
    model = TFBertForSequenceClassification.from_pretrained(model_name)
    tokenizer = BertTokenizer.from_pretrained(model_name)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_text = "This is a test sentence for benchmarking BERT model performance."
    inputs = tokenizer(
        test_text, return_tensors="tf", padding=True, truncation=True, max_length=128
    )

    # é¢„çƒ­
    print(f"é¢„çƒ­è¿è¡Œ {num_warmup} æ¬¡...")
    for _ in range(num_warmup):
        _ = model(inputs)

    # åŸºå‡†æµ‹è¯•
    print(f"åŸºå‡†æµ‹è¯•è¿è¡Œ {num_runs} æ¬¡...")
    latencies = []

    for i in range(num_runs):
        start_time = time.perf_counter()
        _ = model(inputs)
        end_time = time.perf_counter()

        latency_ms = (end_time - start_time) * 1000
        latencies.append(latency_ms)

        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i + 1}/{num_runs}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    latencies = np.array(latencies)
    results = {
        "framework": "TensorFlow",
        "model": model_name,
        "num_runs": num_runs,
        "mean_latency_ms": float(np.mean(latencies)),
        "std_latency_ms": float(np.std(latencies)),
        "min_latency_ms": float(np.min(latencies)),
        "max_latency_ms": float(np.max(latencies)),
        "p50_latency_ms": float(np.percentile(latencies, 50)),
        "p95_latency_ms": float(np.percentile(latencies, 95)),
        "p99_latency_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": 1000.0 / np.mean(latencies),
    }

    print(f"\nç»“æœ:")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_latency_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_latency_ms']:.2f} ms")
    print(f"  P99å»¶è¿Ÿ: {results['p99_latency_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results, model, tokenizer


def convert_to_onnx_with_optimum(model_name, output_dir):
    """
    ä½¿ç”¨Optimumå°†HuggingFaceæ¨¡å‹è½¬æ¢ä¸ºONNX

    å‚æ•°:
        model_name: HuggingFaceæ¨¡å‹åç§°
        output_dir: ONNXæ¨¡å‹è¾“å‡ºç›®å½•
    """
    print_section(f"ä½¿ç”¨Optimumè½¬æ¢ä¸ºONNX ({model_name})")

    from optimum.onnxruntime import ORTModelForSequenceClassification
    from transformers import AutoTokenizer

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"è½¬æ¢æ¨¡å‹åˆ°: {output_path}")
    print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")

    start_time = time.time()

    # Optimumä¼šè‡ªåŠ¨å¤„ç†è½¬æ¢
    ort_model = ORTModelForSequenceClassification.from_pretrained(
        model_name, export=True  # è‡ªåŠ¨å¯¼å‡ºä¸ºONNX
    )

    # ä¿å­˜ONNXæ¨¡å‹
    ort_model.save_pretrained(output_path)

    # åŒæ—¶ä¿å­˜tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_path)

    conversion_time = time.time() - start_time

    # è·å–æ¨¡å‹å¤§å°
    onnx_file = output_path / "model.onnx"
    if onnx_file.exists():
        model_size_mb = onnx_file.stat().st_size / (1024 * 1024)
        print(f"\nâœ“ ONNXè½¬æ¢æˆåŠŸ!")
        print(f"  è½¬æ¢æ—¶é—´: {conversion_time:.2f} ç§’")
        print(f"  ONNXæ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
        print(f"  ä¿å­˜è·¯å¾„: {onnx_file}")
    else:
        print("è­¦å‘Š: ONNXæ–‡ä»¶æœªæ‰¾åˆ°")
        model_size_mb = 0

    return {
        "conversion_time_sec": conversion_time,
        "model_size_mb": model_size_mb,
        "output_path": str(output_path),
    }


def benchmark_onnx_bert(model_dir, num_runs=100, num_warmup=10):
    """
    ONNX Runtime BERTåŸºå‡†æµ‹è¯•

    å‚æ•°:
        model_dir: ONNXæ¨¡å‹ç›®å½•
        num_runs: æµ‹è¯•è¿è¡Œæ¬¡æ•°
        num_warmup: é¢„çƒ­è¿è¡Œæ¬¡æ•°
    """
    print_section(f"ONNX Runtime BERT åŸºå‡†æµ‹è¯•")

    from optimum.onnxruntime import ORTModelForSequenceClassification
    from transformers import AutoTokenizer

    # åŠ è½½ONNXæ¨¡å‹
    print(f"åŠ è½½ONNXæ¨¡å‹: {model_dir}")
    model = ORTModelForSequenceClassification.from_pretrained(model_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_text = "This is a test sentence for benchmarking BERT model performance."
    inputs = tokenizer(
        test_text, return_tensors="pt", padding=True, truncation=True, max_length=128
    )

    # é¢„çƒ­
    print(f"é¢„çƒ­è¿è¡Œ {num_warmup} æ¬¡...")
    for _ in range(num_warmup):
        _ = model(**inputs)

    # åŸºå‡†æµ‹è¯•
    print(f"åŸºå‡†æµ‹è¯•è¿è¡Œ {num_runs} æ¬¡...")
    latencies = []

    for i in range(num_runs):
        start_time = time.perf_counter()
        _ = model(**inputs)
        end_time = time.perf_counter()

        latency_ms = (end_time - start_time) * 1000
        latencies.append(latency_ms)

        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i + 1}/{num_runs}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    latencies = np.array(latencies)
    results = {
        "framework": "ONNX Runtime (Optimum)",
        "model_dir": str(model_dir),
        "num_runs": num_runs,
        "mean_latency_ms": float(np.mean(latencies)),
        "std_latency_ms": float(np.std(latencies)),
        "min_latency_ms": float(np.min(latencies)),
        "max_latency_ms": float(np.max(latencies)),
        "p50_latency_ms": float(np.percentile(latencies, 50)),
        "p95_latency_ms": float(np.percentile(latencies, 95)),
        "p99_latency_ms": float(np.percentile(latencies, 99)),
        "throughput_samples_per_sec": 1000.0 / np.mean(latencies),
    }

    print(f"\nç»“æœ:")
    print(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_latency_ms']:.2f} ms")
    print(f"  P95å»¶è¿Ÿ: {results['p95_latency_ms']:.2f} ms")
    print(f"  P99å»¶è¿Ÿ: {results['p99_latency_ms']:.2f} ms")
    print(f"  ååé‡: {results['throughput_samples_per_sec']:.2f} samples/sec")

    return results


def generate_report(env_info, tf_results, onnx_results, conversion_info, output_file):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
    print_section("ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")

    # è®¡ç®—æ€§èƒ½æå‡
    speedup = tf_results["mean_latency_ms"] / onnx_results["mean_latency_ms"]
    p95_speedup = tf_results["p95_latency_ms"] / onnx_results["p95_latency_ms"]
    p99_speedup = tf_results["p99_latency_ms"] / onnx_results["p99_latency_ms"]
    throughput_speedup = (
        onnx_results["throughput_samples_per_sec"] / tf_results["throughput_samples_per_sec"]
    )

    # Format values for table
    tf_mean = tf_results["mean_latency_ms"]
    onnx_mean = onnx_results["mean_latency_ms"]
    tf_p95 = tf_results["p95_latency_ms"]
    onnx_p95 = onnx_results["p95_latency_ms"]
    tf_p99 = tf_results["p99_latency_ms"]
    onnx_p99 = onnx_results["p99_latency_ms"]
    tf_throughput = tf_results["throughput_samples_per_sec"]
    onnx_throughput = onnx_results["throughput_samples_per_sec"]

    report = f"""# BERTæ¨¡å‹ TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

## ç¯å¢ƒä¿¡æ¯

- **TensorFlow**: {env_info['tensorflow']}
- **NumPy**: {env_info['numpy']}
- **ONNX Runtime**: {env_info['onnxruntime']}
- **Optimum**: {env_info['optimum']}
- **Transformers**: {env_info['transformers']}
- **Protobuf**: {env_info.get('protobuf', 'N/A')}

## ONNXè½¬æ¢ä¿¡æ¯

- **è½¬æ¢æ–¹æ³•**: HuggingFace Optimum
- **è½¬æ¢æ—¶é—´**: {conversion_info['conversion_time_sec']:.2f} ç§’
- **ONNXæ¨¡å‹å¤§å°**: {conversion_info['model_size_mb']:.2f} MB
- **ä¼˜åŠ¿**: æ— protobufç‰ˆæœ¬å†²çªï¼ŒåŸç”Ÿæ”¯æŒTransformersæ¨¡å‹

## æ€§èƒ½å¯¹æ¯”

### TensorFlow

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| å¹³å‡å»¶è¿Ÿ | {tf_results['mean_latency_ms']:.2f} ms |
| P50å»¶è¿Ÿ | {tf_results['p50_latency_ms']:.2f} ms |
| P95å»¶è¿Ÿ | {tf_results['p95_latency_ms']:.2f} ms |
| P99å»¶è¿Ÿ | {tf_results['p99_latency_ms']:.2f} ms |
| ååé‡ | {tf_results['throughput_samples_per_sec']:.2f} samples/sec |

### ONNX Runtime (via Optimum)

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| å¹³å‡å»¶è¿Ÿ | {onnx_results['mean_latency_ms']:.2f} ms |
| P50å»¶è¿Ÿ | {onnx_results['p50_latency_ms']:.2f} ms |
| P95å»¶è¿Ÿ | {onnx_results['p95_latency_ms']:.2f} ms |
| P99å»¶è¿Ÿ | {onnx_results['p99_latency_ms']:.2f} ms |
| ååé‡ | {onnx_results['throughput_samples_per_sec']:.2f} samples/sec |

### æ€§èƒ½æå‡

| æŒ‡æ ‡ | TensorFlow | ONNX Runtime | æå‡å€æ•° |
|------|-----------|--------------|---------|
| **å¹³å‡å»¶è¿Ÿ** | {tf_mean:.2f} ms | {onnx_mean:.2f} ms | \
**{speedup:.2f}x** ğŸš€ |
| **P95å»¶è¿Ÿ** | {tf_p95:.2f} ms | {onnx_p95:.2f} ms | \
**{p95_speedup:.2f}x** |
| **P99å»¶è¿Ÿ** | {tf_p99:.2f} ms | {onnx_p99:.2f} ms | \
**{p99_speedup:.2f}x** |
| **ååé‡** | {tf_throughput:.2f} samples/s | {onnx_throughput:.2f} \
samples/s | **{throughput_speedup:.2f}x** ğŸ“ˆ |

## ç»“è®º

ä½¿ç”¨HuggingFace Optimumè¿›è¡ŒONNXè½¬æ¢å’Œæ¨ç†ï¼š

âœ… **æ— ä¾èµ–å†²çª**: å®Œç¾å…¼å®¹TensorFlow 2.20å’Œprotobuf 5.x
âœ… **æ€§èƒ½æå‡**: ONNX Runtimeæ¯”TensorFlowå¿« **{speedup:.2f}å€**
âœ… **æ˜“ç”¨æ€§**: è‡ªåŠ¨åŒ–è½¬æ¢å’Œä¼˜åŒ–ï¼ŒAPIç®€æ´
âœ… **ç”Ÿäº§å°±ç»ª**: é€‚åˆéƒ¨ç½²åˆ°CPUæ¨ç†ç¯å¢ƒ

## ç”Ÿæˆæ—¶é—´

{time.strftime('%Y-%m-%d %H:%M:%S')}
"""

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    print(f"\næ€§èƒ½æå‡æ€»ç»“:")
    print(f"  ONNX Runtime æ¯” TensorFlow å¿« {speedup:.2f}x ğŸš€")

    return report


def main():
    parser = argparse.ArgumentParser(
        description="BERT TensorFlow vs ONNX Runtime æ€§èƒ½å¯¹æ¯” (ä½¿ç”¨Optimum)"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="bert-base-uncased",
        help="HuggingFaceæ¨¡å‹åç§° (default: bert-base-uncased)",
    )
    parser.add_argument("--num-runs", type=int, default=100, help="åŸºå‡†æµ‹è¯•è¿è¡Œæ¬¡æ•° (default: 100)")
    parser.add_argument("--num-warmup", type=int, default=10, help="é¢„çƒ­è¿è¡Œæ¬¡æ•° (default: 10)")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="results/bert_optimum_benchmark",
        help="è¾“å‡ºç›®å½• (default: results/bert_optimum_benchmark)",
    )
    parser.add_argument("--skip-tf", action="store_true", help="è·³è¿‡TensorFlowåŸºå‡†æµ‹è¯•")
    parser.add_argument("--skip-onnx", action="store_true", help="è·³è¿‡ONNXåŸºå‡†æµ‹è¯•")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥ç¯å¢ƒ
    env_info = check_environment()

    # TensorFlowåŸºå‡†æµ‹è¯•
    if not args.skip_tf:
        tf_results, tf_model, tokenizer = benchmark_tensorflow_bert(
            model_name=args.model_name, num_runs=args.num_runs, num_warmup=args.num_warmup
        )
    else:
        print("è·³è¿‡TensorFlowåŸºå‡†æµ‹è¯•")
        tf_results = None

    # ONNXè½¬æ¢
    onnx_model_dir = output_dir / "onnx_model"
    conversion_info = convert_to_onnx_with_optimum(
        model_name=args.model_name, output_dir=onnx_model_dir
    )

    # ONNXåŸºå‡†æµ‹è¯•
    if not args.skip_onnx:
        onnx_results = benchmark_onnx_bert(
            model_dir=onnx_model_dir, num_runs=args.num_runs, num_warmup=args.num_warmup
        )
    else:
        print("è·³è¿‡ONNXåŸºå‡†æµ‹è¯•")
        onnx_results = None

    # ä¿å­˜ç»“æœ
    results = {
        "environment": env_info,
        "tensorflow": tf_results,
        "onnx_runtime": onnx_results,
        "conversion": conversion_info,
    }

    results_file = output_dir / "results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    # ç”ŸæˆæŠ¥å‘Š
    if tf_results and onnx_results:
        report_file = output_dir / "benchmark_report.md"
        generate_report(env_info, tf_results, onnx_results, conversion_info, report_file)

    print_section("æµ‹è¯•å®Œæˆï¼")
    print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()
