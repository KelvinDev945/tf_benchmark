# TensorFlow Multi-Engine CPU Inference Benchmark

> Comprehensive performance comparison of TensorFlow, TFLite, ONNX Runtime, and OpenVINO on CPU architectures

## âš ï¸ é‡è¦è¯´æ˜ / Important Note

**é¡¹ç›®æ ¸å¿ƒä»ä»¥å›¾åƒæ¨¡å‹ä¸ºä¸»ï¼Œä½†å·²é€šè¿‡ TensorFlow Hub æ¢å¤ BERT æ–‡æœ¬æ¨¡å‹çš„å¯é€‰æ”¯æŒã€‚**

The benchmark focuses on image workloads by default, while optional BERT text pipelines are available without requiring HuggingFace `transformers` / `datasets`. The TensorFlow engine continues to support:
- âœ… Native Keras models (`tf.keras.Sequential`, `tf.keras.Model`)
- âœ… TensorFlow SavedModel format
- âœ… TensorFlow Hub BERT encoders + è½»é‡çº§æ–‡æœ¬æ•°æ®ç®¡çº¿

## âœ¨ Features

- ğŸš€ **Multi-Engine Support**: TensorFlow, TFLite, ONNX Runtime, and OpenVINO
- ğŸ¯ **Real-World Datasets**: ImageNet-1Kã€CIFAR-10/100 å›¾åƒæ•°æ®é›† + å†…ç½®è½»é‡çº§æ–‡æœ¬æ ·æœ¬
- âš¡ **Multiple Optimizations**: XLA JIT, mixed precision, quantization (INT8, FP16)
- ğŸ“Š **Comprehensive Metrics**: Latency (P50/P95/P99), throughput, CPU/memory usage
- ğŸ³ **Docker Support**: Containerized execution for reproducibility
- ğŸ”§ **Multi-Architecture**: x86_64 and ARM64 support
- ğŸ“ˆ **Rich Reporting**: Automated HTML reports with 10+ visualizations

## ğŸ“‹ Supported Engines and Models

| Engine | Configurations | x86_64 | ARM64 |
|--------|----------------|--------|-------|
| **TensorFlow** | baseline, xla, threads, mixed_precision, best_combo | âœ… | âœ… |
| **TFLite** | float32, dynamic_range, int8, float16 | âœ… | âœ… |
| **ONNX Runtime** | default, optimized, quantized | âœ… | âœ… |
| **OpenVINO** | fp32, fp16, int8, dynamic | âœ… | âŒ |

### Supported Models

**Image Classification (é»˜è®¤æ”¯æŒ):**
- MobileNetV2
- ResNet50
- EfficientNetB0
- InceptionV3
- VGG16
**Text Understanding (å¯é€‰ï¼ŒåŸºäº TensorFlow Hub):**
- BERT Base (uncased)
- è‡ªå¸¦è½»é‡çº§ `TextDatasetLoader`ï¼ˆæ— éœ€ HuggingFace æ•°æ®é›†ï¼‰

### BERT Demo (TensorFlow Hub)

```bash
# æŒ‚è½½æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œé¿å…æ¯æ¬¡é‡æ–°ä¸‹è½½ TF Hub æ¨¡å‹
docker run --rm \
    -v ~/.cache/tfhub:/root/.cache/tfhub \
    -v $(pwd):/workspace -w /workspace \
    --entrypoint python3 tf-cpu-benchmark:uv \
    scripts/demo_bert_tf_only.py
```

> è¯´æ˜ï¼šè„šæœ¬ä¸æ ¸å¿ƒä»£ç ä¼šä¼˜å…ˆä» `~/.cache/tfhub`ï¼ˆå¯é€šè¿‡ `TFHUB_CACHE_DIR` è¦†ç›–ï¼‰è¯»å–æ¨¡å‹ï¼›è‹¥ç¼“å­˜ç¼ºå¤±ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶å†™å›è¯¥ç›®å½•ã€‚
>
> è¿è¡Œå®Œæˆåï¼ŒåŸºå‡†ç»“æœä¸ Markdown æŠ¥å‘Šä¼šå†™å…¥ `results/bert_tf_demo/`ã€‚é¦–æ¬¡æ‰§è¡Œéœ€è¦è”ç½‘ä¸‹è½½çº¦ 430â€¯MB çš„ TF Hub æ¨¡å—ï¼›åç»­è¿è¡Œåªè¦æŒ‚è½½ç›¸åŒç¼“å­˜ç›®å½•å³å¯å¤ç”¨ã€‚

å¦‚éœ€ä¿®æ”¹æ‰¹å¤§å°ã€åºåˆ—é•¿åº¦æˆ–è¿­ä»£æ¬¡æ•°ï¼Œå¯ç¼–è¾‘ `scripts/demo_bert_tf_only.py` é¡¶éƒ¨çš„ `BATCH_SIZE`ã€`SEQ_LENGTH`ã€`NUM_WARMUP` ä¸ `NUM_TEST` é…ç½®ï¼›`src/models.ModelLoader.load_text_model()` åŒæ ·å¤ç”¨ä¸Šè¿°ç¼“å­˜ç›®å½•ï¼Œå¯ç›´æ¥åœ¨è‡ªå®šä¹‰æµç¨‹ä¸­åŠ è½½ BERT åˆ†ç±»å™¨ã€‚

## ğŸš€ Quick Start

### Full Benchmark Suite

```bash
# 1. Build Docker image (optional)
./scripts/build_images.sh

# 2. Run comprehensive benchmark (all models + all engines)
./scripts/run_full_benchmark.sh standard

# 3. View consolidated results
cat results/full_benchmark_*/consolidated_report/consolidated_report.md
```

### Docker Quick Start

```bash
# 1. Build Docker image
./scripts/build_images.sh

# 2. Run benchmark
docker run --rm -v $(pwd)/results:/app/results \
    tf-cpu-benchmark:latest \
    src/main.py --mode quick

# 3. View results
open results/latest/report/report.html
```

### âš¡ Docker with uv (Optimized - 2-3x Faster Build)

**NEW**: Docker image optimized with [uv](https://github.com/astral-sh/uv) package manager for ultra-fast builds!

```bash
# Build optimized Docker image with uv
docker build -t tf-cpu-benchmark:uv -f docker/Dockerfile .

# Run quick environment test
docker run --rm -v $(pwd)/results:/app/results -v $(pwd)/scripts:/app/scripts \
    tf-cpu-benchmark:uv scripts/test_docker_env.py

# View test results
cat results/docker_uv_test/mobilenet_v2_results.json
```

**Performance**:
- Build time: ~1-2 minutes (vs 3-5 minutes with pip) - **2-3x faster** âš¡
- Package installation: ~25 seconds (vs 60-120 seconds) - **up to 5x faster**
- MobileNetV2 inference: 85.8ms latency, 11.66 samples/sec

See [DOCKER_UV_TEST_RESULTS.md](DOCKER_UV_TEST_RESULTS.md) for detailed benchmarks.

## ğŸ“¦ Installation

### Option 1: Docker (Recommended)

**Prerequisites:**
- Docker 20.10+
- 8GB+ RAM
- 20GB+ disk space

**Build and Run:**

```bash
# Clone repository
git clone https://github.com/yourusername/tf-cpu-benchmark.git
cd tf-cpu-benchmark

# Build Docker image
./scripts/build_images.sh

# Run standard benchmark
docker run --rm \
    -v $(pwd)/results:/app/results \
    -v $(pwd)/configs:/app/configs \
    tf-cpu-benchmark:latest \
    src/main.py --mode standard
```

### Option 2: Local Installation

**Prerequisites:**
- Python 3.11+
- pip 23.0+
- 16GB+ RAM (for full benchmark)

**Install Dependencies:**

```bash
# Clone repository
git clone https://github.com/yourusername/tf-cpu-benchmark.git
cd tf-cpu-benchmark

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install git hooks for code style checks
pip install pre-commit
pre-commit install

# Note: OpenVINO is only available on x86_64
# On x86_64, additionally run:
pip install openvino==2023.2.0 openvino-dev==2023.2.0
```

**Run Benchmark:**

```bash
python src/main.py --config configs/benchmark_config.yaml --mode standard
```

### Option 3: CPU-Optimized TensorFlow (Best Performance)

**âš¡ è·å¾—2-4å€æ€§èƒ½æå‡ï¼**

The default TensorFlow pip package is a generic build. For optimal CPU performance, compile TensorFlow from source with CPU-specific optimizations.

**Quick Start - Intel Optimized TensorFlow:**

```bash
# Easiest option: Use Intel's pre-optimized build
pip uninstall tensorflow
pip install intel-tensorflow==2.20.0

# Test performance improvement
python3 scripts/benchmark_xla_mixed_precision.py --model-type mobilenet_v2 --num-runs 30
```

**Expected Performance Gains:**
- MobileNetV2 inference: 1.8-3.8x faster
- ResNet50 inference: 1.5-3.0x faster
- Matrix operations: 2.0-4.0x faster (with AVX512)

**Build from Source (Maximum Performance):**

For maximum performance, compile TensorFlow with your CPU's specific instruction sets (AVX2, AVX512, FMA):

```bash
# See detailed guide
cat TENSORFLOW_CPU_OPTIMIZATION.md

# Quick example for AVX512 CPUs:
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
bazel build --config=opt \
    --config=mkl \
    --copt=-march=native \
    --copt=-mavx512f \
    //tensorflow/tools/pip_package:build_pip_package
```

ğŸ“– **Full Guide**: See [TENSORFLOW_CPU_OPTIMIZATION.md](TENSORFLOW_CPU_OPTIMIZATION.md) for:
- CPU instruction set detection
- Platform-specific build commands (Intel/AMD/ARM)
- Docker-based compilation
- Performance benchmarking before/after
- Troubleshooting

**When to Use CPU-Optimized TensorFlow:**
- âœ… Production deployments requiring maximum CPU performance
- âœ… Training workloads on CPU servers
- âœ… When you control the deployment hardware
- âŒ Cross-platform distribution (use generic build)
- âŒ Quick prototyping (use ONNX Runtime instead)

## ğŸ”§ Configuration

The benchmark is configured via `configs/benchmark_config.yaml`. Key sections:

### Benchmark Parameters

```yaml
benchmark:
  warmup_iterations: 50      # Number of warmup runs
  test_iterations: 200       # Number of test runs
  repeat_runs: 5             # Repeat entire benchmark N times
  confidence_level: 0.95     # Statistical confidence level
```

### Testing Modes

Three predefined modes for different use cases:

| Mode | Warmup | Test Iterations | Repeat Runs | Use Case |
|------|--------|-----------------|-------------|----------|
| **quick** | 10 | 50 | 1 | Fast verification during development |
| **standard** | 50 | 200 | 3 | Balanced testing (default) |
| **full** | 100 | 500 | 5 | Comprehensive testing for publication |

### Engine Configuration

Each engine can be enabled/disabled and configured:

```yaml
engines:
  tensorflow:
    enabled: true
    configs:
      - name: baseline
        xla: false
        mixed_precision: false
      - name: xla
        xla: true
      # ... more configs
```

### Dataset Configuration

```yaml
dataset:
  image:
    name: "imagenet-1k"
    split: "validation"
    num_samples: 5000
  text:
    name: "glue"
    subset: "sst2"
    num_samples: 1000
```

## ğŸ¯ Usage

### Full Benchmark Suite

**Run All Models + All Engines**:
```bash
# Quick verification (30 minutes)
./scripts/run_full_benchmark.sh quick

# Standard benchmark (2-4 hours, recommended)
./scripts/run_full_benchmark.sh standard

# Full comprehensive benchmark (6-10 hours)
./scripts/run_full_benchmark.sh full
```

**What Gets Tested**:
1. Image modelsï¼ˆMobileNetV2ã€ResNet50ã€EfficientNetB0 ç­‰ï¼‰
2. å¤šå¼•æ“é…ç½®ï¼ˆTensorFlow / TFLite / ONNX Runtime / OpenVINOï¼‰
3. æ‰¹é‡å¤§å°ä¸é‡åŒ–ç­–ç•¥åˆ†æ
4. ç»Ÿä¸€æŠ¥å‘Šç”Ÿæˆï¼ˆHTML / Markdown / å›¾è¡¨ï¼‰

### Basic Usage

```bash
# Quick test (5 minutes)
python src/main.py --mode quick

# Standard benchmark (1-2 hours)
python src/main.py --mode standard

# Full benchmark (4-6 hours)
python src/main.py --mode full
```

### Advanced Options

```bash
# Specify custom config
python src/main.py --config configs/custom_config.yaml

# Test specific engines only
python src/main.py --engines tensorflow,tflite

# Test specific models only
python src/main.py --models mobilenet_v2,resnet50

# Custom output directory
python src/main.py --output ./my_results

# Resume from checkpoint
python src/main.py --resume

# Skip report generation
python src/main.py --no-report
```

### Using Docker

```bash
# Quick test
docker run --rm -v $(pwd)/results:/app/results \
    tf-cpu-benchmark:latest src/main.py --mode quick

# Custom configuration
docker run --rm \
    -v $(pwd)/results:/app/results \
    -v $(pwd)/configs:/app/configs \
    tf-cpu-benchmark:latest \
    src/main.py --config /app/configs/custom_config.yaml

# OpenVINO only (x86_64)
docker run --rm -v $(pwd)/results:/app/results \
    tf-cpu-benchmark-openvino:latest \
    src/main.py --engines openvino --mode standard
```

## ğŸ“Š Output and Reports

### Directory Structure

After running a benchmark, results are organized as follows:

```
results/
â””â”€â”€ 20250106_143022/           # Timestamp
    â”œâ”€â”€ results.json           # Raw results (JSON)
    â”œâ”€â”€ results.csv            # Results table (CSV)
    â”œâ”€â”€ system_info.json       # System information
    â”œâ”€â”€ benchmark.log          # Detailed logs
    â”œâ”€â”€ checkpoint.json        # Resume checkpoint
    â””â”€â”€ report/
        â”œâ”€â”€ report.html        # Interactive HTML report
        â”œâ”€â”€ report.md          # Markdown report
        â”œâ”€â”€ recommendations.txt # Best configurations
        â””â”€â”€ plots/
            â”œâ”€â”€ throughput_comparison.png
            â”œâ”€â”€ latency_boxplot.png
            â”œâ”€â”€ batch_size_analysis.png
            â””â”€â”€ ... (10+ charts)
```

### Generated Reports

**HTML Report** includes:
- Executive summary with key findings
- System configuration details
- Image model performance comparison
- Text model performance comparison
- Engine comparison analysis
- Quantization analysis
- Recommended configurations by scenario
- Interactive visualizations

**Visualizations** (10+ charts):
- Throughput comparison bar charts
- Latency distribution box plots
- Batch size scaling curves
- Sequence length impact (text models)
- Speedup radar charts
- Resource efficiency scatter plots
- Quantization tradeoff analysis
- Model size comparison
- Confidence interval error bars
- Comprehensive ranking heatmap

## ğŸ“ˆ Example Results

### Expected Performance (Intel i9-13900K, x86_64)

| Engine | Model | Batch Size | Throughput (samples/sec) | Latency P50 (ms) |
|--------|-------|------------|--------------------------|------------------|
| TensorFlow (XLA) | MobileNetV2 | 32 | ~1500 | ~21 |
| TFLite (INT8) | MobileNetV2 | 32 | ~2000 | ~16 |
| ONNX (optimized) | MobileNetV2 | 32 | ~1800 | ~18 |
| OpenVINO (INT8) | MobileNetV2 | 32 | ~2500 | ~13 |

*Actual performance varies by hardware*

## ğŸ—ï¸ Project Structure

```
tf-cpu-benchmark/
â”œâ”€â”€ docker/                    # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ Dockerfile.openvino
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ config/               # Configuration management
â”‚   â”œâ”€â”€ dataset/              # Dataset loaders
â”‚   â”œâ”€â”€ models/               # Model loaders and converters
â”‚   â”œâ”€â”€ engines/              # Inference engines
â”‚   â”œâ”€â”€ benchmark/            # Benchmark runner and metrics
â”‚   â””â”€â”€ reporting/            # Report generation
â”œâ”€â”€ configs/                   # Configuration files
â”‚   â””â”€â”€ benchmark_config.yaml
â”œâ”€â”€ scripts/                   # Automation scripts
â”‚   â”œâ”€â”€ build_images.sh
â”‚   â”œâ”€â”€ run_benchmark.sh
â”‚   â””â”€â”€ generate_report.py
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ§ª Testing

Run unit tests:

```bash
# Install test dependencies
pip install -r requirements.txt

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_config.py -v
```

## ğŸ” Troubleshooting

### Common Issues

**Q: Docker build fails with network timeout**

```bash
# Use pip mirror
docker build --build-arg PYPI_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple .
```

**Q: OpenVINO not working on ARM64**

This is expected. OpenVINO only supports x86_64 architecture.

**Q: Out of memory error**

```bash
# Reduce batch sizes or samples in config
# Edit configs/benchmark_config.yaml:
batch_sizes: [1, 4, 8]  # Instead of [1, 4, 8, 16, 32]
```

**Q: Permission denied on scripts**

```bash
chmod +x scripts/*.sh
```

### Testing Environment

- **TensorFlow**: 2.20.0
- **Python**: 3.11
- **Docker Image**: tf-cpu-benchmark:latest
- **Models**: google-bert/bert-base-uncased
- **Dataset**: glue/sst2 (validation split)

## ğŸ›£ï¸ Roadmap

- [ ] Phase 1: Infrastructure âœ… (Current)
- [ ] Phase 2: Data and Model Loaders (Day 3-4)
- [ ] Phase 3: Inference Engines (Day 5-7)
- [ ] Phase 4: Benchmark Core (Day 8-10)
- [ ] Phase 5: Reporting and Visualization (Day 11-13)
- [ ] Phase 6: Documentation and Automation (Day 14-15)

See [plan.md](plan.md) for detailed implementation plan.

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Clone repository
git clone https://github.com/yourusername/tf-cpu-benchmark.git
cd tf-cpu-benchmark

# Install development dependencies
pip install -r requirements.txt

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/

# Format code
black src/ tests/
isort src/ tests/

# Type checking
mypy src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [TensorFlow](https://www.tensorflow.org/) team for the excellent ML framework
- [ONNX Runtime](https://onnxruntime.ai/) team for the cross-platform inference engine
- [OpenVINO](https://docs.openvino.ai/) team for the optimized inference toolkit
- [HuggingFace](https://huggingface.co/) for datasets and transformers
- MLPerf for benchmark methodology inspiration

## ğŸ“§ Contact

For questions or feedback:
- Open an issue on [GitHub](https://github.com/yourusername/tf-cpu-benchmark/issues)
- Email: your.email@example.com

---

**Project Status**: Phase 1 - Infrastructure Setup âœ…

Generated with [Claude Code](https://claude.ai/code) via [Happy](https://happy.engineering)
