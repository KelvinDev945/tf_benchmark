name: Integration Tests

on:
  schedule:
    # Run daily at 2:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: false
        default: 'integration'
        type: choice
        options:
          - integration
          - slow
          - all
  push:
    branches:
      - master
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements.txt'

jobs:
  integration-tests:
    name: Integration Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 60

    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential \
            libhdf5-dev \
            pkg-config

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Determine test markers
        id: markers
        run: |
          test_type="${{ github.event.inputs.test_type }}"
          if [ -z "$test_type" ]; then
            test_type="integration"
          fi

          case "$test_type" in
            integration)
              marker="integration and not slow"
              ;;
            slow)
              marker="slow"
              ;;
            all)
              marker="integration or slow"
              ;;
            *)
              marker="integration and not slow"
              ;;
          esac

          echo "marker=$marker" >> $GITHUB_OUTPUT
          echo "Test marker: $marker"

      - name: Run integration tests
        env:
          HF_HOME: /tmp/huggingface
          TRANSFORMERS_CACHE: /tmp/huggingface/transformers
          HF_DATASETS_CACHE: /tmp/huggingface/datasets
        run: |
          echo "## ðŸ§ª Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Type:** ${{ steps.markers.outputs.marker }}" >> $GITHUB_STEP_SUMMARY
          echo "**Python Version:** ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run tests with coverage
          pytest tests/ -v \
            -m "${{ steps.markers.outputs.marker }}" \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --tb=short \
            --durations=10 | tee test-output.txt

          # Extract summary
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -20 test-output.txt >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-coverage-reports-py${{ matrix.python-version }}
          path: |
            coverage.xml
            htmlcov/
            test-output.txt
          retention-days: 30

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          file: ./coverage.xml
          flags: integration
          name: integration-py${{ matrix.python-version }}
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Generate test report
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Coverage Report" >> $GITHUB_STEP_SUMMARY
          coverage report --format=markdown >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Coverage report not available" >> $GITHUB_STEP_SUMMARY

  benchmark-quick:
    name: Quick Benchmark Test
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run quick benchmark
        env:
          HF_HOME: /tmp/huggingface
          TRANSFORMERS_CACHE: /tmp/huggingface/transformers
          HF_DATASETS_CACHE: /tmp/huggingface/datasets
        run: |
          echo "## ðŸš€ Quick Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Create a minimal config for quick testing
          cat > /tmp/quick_config.yaml << 'EOF'
          model:
            name: "google-bert/bert-base-uncased"
            task: "text-classification"
            num_labels: 2

          dataset:
            name: "glue"
            subset: "sst2"
            split: "validation"
            num_samples: 50

          benchmark:
            batch_sizes: [1, 8]
            num_iterations: 10
            warmup_iterations: 2
            enable_profiling: false

          engines:
            - tensorflow
            - tflite_float32

          output:
            results_dir: "results"
            save_models: false
            generate_report: true
          EOF

          # Run quick benchmark (if main.py supports it)
          python src/main.py --config /tmp/quick_config.yaml || {
            echo "âš ï¸ Quick benchmark not available or failed" >> $GITHUB_STEP_SUMMARY
            echo "This is expected if the codebase doesn't support config-based runs yet" >> $GITHUB_STEP_SUMMARY
            exit 0
          }

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quick-benchmark-results
          path: |
            results/
          retention-days: 30

  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [integration-tests, benchmark-quick]
    if: failure() && github.event_name == 'schedule'

    steps:
      - name: Create issue for failed tests
        uses: actions/github-script@v7
        with:
          script: |
            const title = 'ðŸ”´ Integration Tests Failed';
            const body = `## Integration Test Failure

            The scheduled integration tests have failed.

            **Run Date:** ${new Date().toISOString()}
            **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

            ### Action Required

            1. Review the test failures in the workflow run
            2. Fix any broken tests or code issues
            3. Verify the fix with a manual workflow run
            4. Close this issue once resolved

            ### Jobs Status

            - Integration Tests: ${{ needs.integration-tests.result }}
            - Quick Benchmark: ${{ needs.benchmark-quick.result }}

            See the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed logs.
            `;

            // Check if similar issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'testing,integration'
            });

            const existingIssue = issues.data.find(issue => issue.title === title);

            if (!existingIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['testing', 'integration', 'bug']
              });
            }
